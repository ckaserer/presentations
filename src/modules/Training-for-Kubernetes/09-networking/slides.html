<section>
    <section class="no_bg">
        <h2>Kubernetes Networking</h2>
    </section>

    <section class="no_bg">
        <h2>Kubernetes Network Model</h2>
        <img src="images/kube-networking.png" alt="Kube Networking">
        <p>Requirements</p>
        <ul>
            <li>Pod &lt;--&gt; Pod without NAT</li>
            <li>Node &lt;--&gt; Pod without NAT</li>
            <li>Pod's peers find it at the same IP it finds itself</li>
            <li>Creates a <span class='keyword'>flat network</span>, like VMs</li>
        </ul>
        <aside class="notes">
            <ul>
                <li>Kubernetes does not have a strict opinion on how networking between pods has to be implemented. It only requests the following 3 characteristics of a valid networking implementation:</li>
                <li>(1) All pods can communicate with all other pods without NAT</li>
                <li>(2) All nodes running pods can communicate with all pods (and vice-versa) without NAT</li>
                <li>(3) IP that a pod sees itself as is the same IP that other pods see it as</li>
                <li>In our sample we have two nodes on a subnet 172.10.0.0/16 and all pods are on subnet 10.1.0.0/16 while node1 has subnet 10.1.1.0/24 and node2 has 10.1.2.0/24 reserved for its respective pods.</li>
                <li>According to the requirements (1) pod A needs to be able to reach pod B, pod C and pod D without NAT. (2) Furthermore node 1 and 2 need to be able to reach all pods A to D. (3) Finally pod A sees its own IP as 10.1.1.2 and all other pods in the network see pod A with the same IP 10.1.1.2 and can reach it accordingly.</li>
                <li>To elaborate a bit on the 3rd point: processes running inside any container of pod A see their IP as 10.1.1.2.</li>
                <li>Kube's networking model is most distinct from the Docker native CNM in its priorities; Kubernetes wanted a networking model that most closely resembled a flat network of VMs. By demanding all containers sit on a flat network and can all reach each other, Kubernetes forgoes Docker's secure by default firewalling but makes it very simple to port an application previously distributed across networked VMs into a collection of pods that are nearly identical from a networking perspective. This is the other sense in which kubernetes prioritized its communication model over its security model.</li>
            </ul>
        </aside>
    </section>

    <section class="no_bg">
        <h2>Service</h2>
        <div class="row">
            <div class="col-6">
                <p>Problem:</p>
                <ul>
                    <li>Pods are mortal</li>
                    <li>Pods are never resurrected</li>
                    <li>Pod IP Addr cannot be relied on</li>
                </ul>
                <p>Solution:</p>
                <ul>
                    <li><span class"keyword">Service</span> defines: 
                        <ul>
                            <li>Logical set of Pods</li>
                            <li>Policy how to access them</li>
                        </ul> </li>
                </ul>
            </div>
            <div class="col-6">
                <img src="images/service.png" title="Service">
            </div>
        </div>
        <aside class="notes">
            <ul>
                <li>Just like service tasks managed by Swarm, pods managed by Kubernetes come and go, either from failures, scheduling decisions, updates, or scaling.</li>
                <li>We don't want our application logic to have to watch out for these potentially fast-changing operational details; we'd rather abstract away a pool of identical pods behind a stable endpoint, which is what a Kubernetes service provides.</li>
                <li>A kube service is a fully-fledged object in the Kubernetes API just like Pods, ReplicaSets, and Deployments. They provide stable IP addresses, and support TCP and UDP (TCP by default). They also perform simple randomized load-balancing across Pods, though more advanced load balancing algorithms may be supported in the future. This adds up to a situation where Pods can come and go, and the Service automatically updates and continues to provide that stable networking endpoint.</li>
            </ul>
        </aside>
    </section>

    <section class="no_bg">
        <h2>ClusterIP Services</h2>

        <div class='row'>
            <div class='col-8'>
                <img src='images/clusterip.png'></img>
            </div>
            <div class='col-4'>
                <ul>
                    <li><span class='keyword'>Usecase:</span>
                        <ul>
                            <li>Cluster internal origin</li>
                            <li>Stateless destination</li>
                            <li>Similar to Swarm VIP</li>
                        </ul>
                    </li>
                </ul>
            </div>
        </div>

        <aside class='notes'>
            <ul>
                <li>The most basic Kubernetes service is the clusterIP service. Traffic sent to the service IP and port will get randomly load balanced across all matching pods, on the service's targetPort port.</li>
                <li>Note this is for cluster internal communications only; the service IP will only be reachable from other pods running on the same cluster.</li>
                <li>Also note, the random routing implies this is appropriate only for stateless destination pods.</li>
                <li>Finally, recall that UCP automatically deploys a kube DNS service - so the IP of your clusterIP services, like all services, can be resolved by a DNS lookup of the service name, again from within the originating pod.</li>
            </ul>
        </aside>
    </section>

    <section class="no_bg">
        <h2>NodePort Services</h2>

        <div class='row'>
            <div class='col-8'>
                <img src='images/nodeport.png'></img>
            </div>
            <div class='col-4'>
                <ul>
                    <li><span class='keyword'>Usecase:</span>
                        <ul>
                            <li>Cluster external origin</li>
                            <li>Stateless destination</li>
                            <li>Similar to Swarm L4 mesh</li>
                        </ul>
                    </li>
                </ul>
            </div>
        </div>

        <aside class='notes'>
            <ul>
                <li>Building off of the clusterIP service is the nodePort service, intended for routing traffic from outside the cluster to your pods.</li>
                <li>nodePort services automatically create clusterIP services that work exactly as described above; the nodePort service itself listens on a randomly assigned port on every node in the cluster, and forwards traffic inbound there to the clusterIP service, which in turn hands it off to the matching pods as above.</li>
                <li>Note that nodePort services listen on EVERY host in the cluster, regardless of what pods are running where; therefore, inbound traffic doesn't need to be targeted at a specific node. Your external load balancer could in principle just fan traffic across the whole cluster on the port selected by your nodePort service, and the Kube services will handle the rest of the routing.</li>
            </ul>
        </aside>
    </section>

    <section class="no_bg">
        <h2>Kubernetes Label Selectors</h2>

        <img src="images/labelselector.png" title="label selector" style='width:70%'>

        <aside class='notes'>
            <ul>
                <li>Since a Kubernetes service is a completely independent object, how do we connect it to the pods it's supposed to route traffic to?</li>
                <li>We address this problem with kube label selectors. Pods bear a metadata -> labels key, beneath which can contain any arbitrary label and value; meanwhile, services bear a selector -> matchLabels key beneath which resides the same label and value combination.</li>
                <li>You already saw this in the exercises when you created replicaSets and deployments; the higher level objects were matched to pods in the exact same way. In the examples you declared the pods simultaneously with the orchestration objects, but any pod, even ones declared separately, with a matching label will get picked up and managed by the replicaSet, deployment or service.</li>
                <li>In order to restrict what objects match what other objects in this way, Kubernetes also offers namespacing functionality for objects; in this workshop we'll just put everything in the default namespace, but in practice you might consider running multiple versions of an app (say dev and staging) in different namespaces, so the dev services don't point at staging pods, or vice versa.</li>
            </ul>
        </aside>

    </section>

    <section class="no_bg">
        <h2>Network Policies</h2>
        <div class="row">
            <div class="col-7">
                <ul>
                    <li>Network policies <span class="keyword">control traffic</span></li>
                    <li>Traffic allowed by default</li>
                    <li>Traffic denied if network policy exist but no rule allows it</li>
                    <li>Independent <span class="keyword">ingress</span> &amp; <span class="keyword">egress</span> rules</li>
                </ul>
            </div>
            <div class="col-5">
                <div class="pre large">apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: ...
  namespace: ...
  ...
spec:
  <span class="red-bg">podSelector:</span> ...
  <span class="red-bg">ingress:</span>
  - ...
  - ...
  <span class="red-bg">egress:</span>
  - ...
  - ...</div>
            </div>
        </div>
        <aside class="notes">
            <ul>
                <li>Network policies control traffic from and to pods</li>
                <li>Kubernetes defines policies but ignores them silently IF no supporting network plugin is installed.</li>
                <li>Calico is such a network plugin that Docker includes "as batteries included" in UCP 3.0. Calico supports network policies and is commercially backed by Tigera.</li>
                <li>Policies work based on labels (on pods). In a policy we have label selectors that define which pods are affected by the policy</li>
                <li>An empty label selector means that ALL pods are included: matchLabels: {}</li>
                <li>In addition to which pods it applies to a policy also defines which direction of traffic is affected: ingress (inbound traffic; who can access the pod) or egress (outbound traffic; where can the pod connect to)</li>
                <li>On the slide we see the template of a network policy with the podSelector and the ingress and egress rules.</li>
                <li>Traffic is allowed unless there is a network policy selecting the pod</li>
                <li>Traffic is denied if there are policies selecting the pod but none of them have rules allowing it</li>
                <li>We can only write rules to ** allow ** traffic</li>
                <li>Traffic is allowed if there is at least one policy allowing it</li>
                <li>Policies can also restrict port numbers and protocols, e.g. 3456/tcp</li>
                <li>Policy rules are additive (logical OR)</li>
                <li>Multiple pod selectors are also additive (logical OR)</li>
                <li>Network policies are scoped to their namespace (the one they're deployed to)</li>
                <li>Network policies add minimal latency (less than 1 ms overhead): http://blog.kubernetes.io/2016/09/high-performance-network-policies-kubernetes.html</li>
            </ul>
        </aside>
    </section>

    <section class="no_bg">
        <h2>Sample Network Policies</h2>
        <img src="images/network-policies.png" alt="Sample Network Policy">
        <aside class='notes'>
            <ul>
                <li>Here we see a sample network policy that allows some traffic while denying other. Only components from the backend tier can communicate with the DB tier but not e.g. frontend components</li>
            </ul>
        </aside>
    </section>

    <section class="no_bg">
        <h2>Kubernetes Networking Planes</h2>

        <ul>
            <li>
                Management:
                <ul>
                    <li>Master to Master: etcd Raft</li>
                    <li>Master to Node: apiserver (TCP 6443) &lt;-&gt; kubelet (TCP 10250)</li>
                </ul>
            </li>
            <li>
                Data &amp; Control:
                <ul>
                    <li>BYO networking</li>
                    <li>See Cluster DNS, <a href='http://bit.ly/2DMmdyt'>http://bit.ly/2DMmdyt</a></li>
                </ul>
            </li>
        </ul>

        <aside class='notes'>
            <ul>
                <li>Like a Swarm, Kubernetes needs a management, control, and data plane to orchestrate fully distributed applications.</li>
                <li>Kube's management plane consists of two distinct parts: etcd replicas across masters maintain a Raft consensus, and masters' apiserver communicates with node's kubelet.</li>
                <li>Unlike Swarm, Kubernetes control and data planes are largely bring-your-own, with the details governed by networking solutions like calico, flannel or weave.</li>
                <li>Also see the Cluster DNS addon for Kubernetes; this will provision a DNS server that registers DNS names for Kubernetes services and pods based on their names and namespaces.</li>
            </ul>
        </aside>

    </section>

    <section class="no_bg">
        <h2>Calico</h2>

        <img src="images/calico-routing.png" alt="bgp / ip in ip">

        <aside class='notes'>
            <ul>
                <li>Every kubernetes network plugin implements L3 routing differently; here we illustrate Calico, since that's the one that ships with UCP by default.</li>
                <li>Calico assigns a /26 subnet to each node in the cluster (a node can get additional /26 subnets as they fill up - but a particular /26 will belong to exactly one host.)</li>
                <li>All pods on a host receive IPs in this /26 subnet allocated by the calico IPAM.</li>
                <li>Each pod's network namespace is connected to the host network namespace via a veth, just like in Docker.</li>
                <li>Calico amends the host's routing table to direct traffic bound for a local pod to the corresponding veth endpoint, named 'cali*'.</li>
                <li>Calico also runs a BGP server (BIRD) in an all-to-all mesh, updating the routing tables of all other nodes in the cluster with information about which subnets have been assigned to which nodes. Then, if any traffic is destined for a subnet not corresponding to the local node, the destination node's IP is inferred from the /26 subnet, and the packet is sent via IP-in-IP.</li>
                <li>In this sense, Calico is using BGP as its networking control plane (compare to gossip in Swarm), and IP in IP as its data plane (compare to VXLAN)</li>
                <li>In the case of very large clusters, the all-to-all BGP mesh can become unperformant. It's also possible to configure calico route reflectors to serve as the 'hub' in a hub-and-spoke control plane, where all nodes route outbound traffic to the route reflectors rather than directly to its destination, and the route reflectors then pass the traffic to its destination. This way, the control plane scales linearly in cluster size rather than like n*n, though at the cost of having a central hub for all intra-cluster communications (needs high bandwidth, potential point of failure).</li>
            </ul>
        </aside>

    </section>

    <section data-background="#00a2a1" class="green_bg">
        <h2><img src="images/icon_task.png" class="moby_icon" alt="icon"> Exercise: Kubernetes Networking</h2>

        <p>Work through</p> 
        <ul>
            <li class='exercise' script='kube-networking.md'>Kubernetes Networking</li>
        </ul>

        <p>In the Exercises book.</p>
        <h2 class="timer"></h2>

    </section> 
    
    <section class="no_bg">
        <h2>Kubernetes Takeaways</h2>
        <ul>
            <li>Kubernetes provides more flexibility in its orchestration objects at the cost of more config</li>
        </ul>
        <aside class='notes'>
            <ul>
                <li>The key strategic things to understand when thinking about kube are its modularity, and its communication model.</li>
                <li>Compared to Swarm, Kube is much more modular, providing a great deal of flexibility in scheduling decisions. However, as is generally true for most software, added flexibility often results in more complicated config.</li>
                <li>Also comparing to swarm, kube offers a potentially powerful communication model via the concept of pods and its globally reachable network model. If your app requires very chatty containers that need interprocess communication or colocation, pods are an excellent design decision offered by kubernetes.</li>
            </ul>
        </aside>
    </section>

    <section class="no_bg">
        <h2>Further Reading</h2>
        <ul>
            <li>Docker &amp; Kubernetes: <a href="https://www.docker.com/kubernetes">https://www.docker.com/kubernetes</a></li>
            <li>Official Kubernetes Docs: <a href="https://kubernetes.io/docs">https://kubernetes.io/docs</a></li>
            <li>Tutorials: <a href="http://bit.ly/2yLGn61">http://bit.ly/2yLGn61</a></li>
            <li>Interactive Tutorials: <a href="https://bit.ly/2rdwIVZ">https://bit.ly/2rdwIVZ</a></li>
            <li>Understanding Kubernetes Networking: <a href="http://bit.ly/2kdI1qQ">http://bit.ly/2kdI1qQ</a></li>
            <li>Kubernetes the Hard Way: <a href="http://bit.ly/29Dq4wC">http://bit.ly/29Dq4wC</a></li>
        </ul>
        <aside class='notes'>
            <ul>
                <li>Additional resources about Kubernetes</li>
            </ul>
        </aside>
    </section>
</section>
