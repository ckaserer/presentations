<!doctype html>
<html>
	<head>
        <!-- Global site tag (gtag.js) - Google Analytics -->
        <script async src="https://www.googletagmanager.com/gtag/js?id=UA-155731101-1"></script>
        <script>
            window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
            gtag('config', 'UA-155731101-1');
        </script>

		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">
        
        <title>Presentation</title>

        <meta name="description" content="Presentation">
        <meta name="author" content="Clemens Kaserer">

		<link rel="stylesheet" href="dist/reset.css">
		<link rel="stylesheet" href="dist/reveal.css">
		<link rel="stylesheet" href="src/css/general.css" id="theme">
        <link rel="stylesheet" href="src/css/custom.css">
        <link rel="stylesheet" href="src/css/cusotm-code.css">

		<!-- Theme used for syntax highlighted code -->
		<link rel="stylesheet" href="plugin/highlight/monokai.css">
	</head>
	<body>

        <!-- Printing and PDF exports -->
        <script>
            var link = document.createElement( 'link' );
            link.rel = 'stylesheet';
            link.type = 'text/css';
            link.href = window.location.search.match( /print-pdf/gi ) ? 'src/css/print/pdf.css' : 'src/css/print/paper.css';
            document.getElementsByTagName( 'head' )[0].appendChild( link );
        </script>

        <!--[if lt IE 9]>
        <script src="lib/js/html5shiv.js"></script>
        <![endif]-->
    </head>
    <body>
        <script src="dist/reveal.js"></script>
		<script src="plugin/notes/notes.js"></script>
		<script src="plugin/markdown/markdown.js"></script>
		<script src="plugin/highlight/highlight.js"></script>
        
        <!--img src="images/logo.png" style="width: auto; height: 50px; position: fixed; top: 30px; right: 30px; z-index: -9999;" alt="logo"-->
        <!--<p style="font-size:12px !important; z-index:10; color:rgb(0, 0, 0); position: fixed; bottom: 30px; left: calc(50% - 8em);"><i>#BottomBannerText</i></p>-->
		<div class="reveal">
            <div class="slides">
                <section>
    <section>
        <h2>Containerization<br>Training</h2>
    </section>
    

    <section class="no_bg">
        <h2>How We Teach</h2>
        <ul>
            <li>We believe in <b>learning by doing</b></li>
            <li>The training is <b>lab driven</b></li>
            <li>Work together!</li>
            <li>Ask questions at any time</li>
        </ul>       

        <aside class="notes">
            <ul>
                <li>This workshop is primarily exercise based</li>
                <li>Lecture will be limited and focus on the high-level concepts, best practices, and ideas we want to tell you about</li>
                <li>Most of our time will be spent on demo exercises, designed to illustrate the usage, syntax, and details of all the tools we explore, and map onto the learning objectives for each chapter</li>
            </ul>
        </aside>
    </section>
    
    <section class="no_bg">
        <h2>Session Logistics</h2>
        <ul>
            <li>2 days duration</li>
            <li>Mostly exercises</li>
            <li>Regular breaks</li>
        </ul>
    </section>

    <section class="no_bg">
        <h2>Assumed Knowledge and Requirements</h2>
        <ul>
            <li>Familiarity with Bash or Powershell</li>
            <li>Bash Cheat Sheet: <a href='http://bit.ly/2mTQr8l'>http://bit.ly/2mTQr8l</a></li>
            <!-- <li>Powershell Cheat Sheet: <a href='https://bit.ly/2EPHxze'>https://bit.ly/2EPHxze</a></li> -->
        </ul>

        <aside class='notes'>
            <p>Basic familiarity with:</p>
            <ul>
                <li>Filesystem navigation and manipulation: ls, cd, mv, cp, rm</li>
                <li>Tooling: ssh, top, chmod, curl, wget</li>
                <li>Package management with yum</li>
                <li>And powershell equivalents</li>
            </ul>
        </aside>
    </section>

    <section class="no_bg">
        <h2>Your Lab Environment</h2>
        <ul>
            <li>You have been given an instance for use in exercises</li>
            <li>Ask instructor for credentials if you don't have them already</li>
        </ul>

        <aside class='notes'>
        </aside>
    </section>

    <section class="no_bg">
        <h2>Course Learning Objectives</h2>

        <p>By the end of this course, learners will be able to</p>
        <ul>
            <li>Assess the advantages of a containerized software development &amp; deployment</li>
            <li>Use Docker engine features necessary for running containerized applications</li>
        </ul>
    </section>  
</section>

<section>
    <section class="no_bg">
        <h2>Introducing Docker</h2>
        <aside class="notes"><h3>An intro module to get students on message with what docker is, and the priorities and concerns of distributed application dev and ops.</h3>
        </aside>
    </section>

    <section class="no_bg">
        <h2>What We Want</h2>

        <p>Ideal software should</p>

        <ul>
            <li>be modular and flexible (devs)</li>
            <li>be easy to migrate (devops)</li>
            <li>be easy to scale, monitor and lifecycle (ops)</li>
            <li>mitigate vulnerabilities (security)</li>
            <li>run cheap (business)</li>
        </ul>

        <aside class='notes'>
            <ul>
                <li>Every stakeholder in the software supply chain has a set of priorities they'd like software to satisfy.</li>
                <li>Developers want flexibility and choice in the components they build application out of; problems like vendor lock-in, technical debt, and tight coupling between components slows the development cycle down and can prevent developers from building the software they want.</li>
                <li>Devops engineers in charge of building and maintaining CI/CD pipelines are primarily responsible for getting applications running across many environments; they'd like to be able to move software easily, and rely that tests passing in one environment won't break in another.</li>
                <li>Operations teams responsible for scaling, maintaining and monitoring software would like to be able to deploy applications easily across a datacenter, and know what to expect from those applications to make monitoring and maintenance simple.</li>
                <li>Security teams want assurances that software not only minimizes attack surfaces, but has built-in failsafes to mitigate compromises when they occur.</li>
                <li>Finally, business interests want to be able to do all of the above as cheaply as possible, by making most efficient use of the compute resources purchased to run it.</li>
                <li>These are many and varied priorities - but containerization is such a success because it speaks to all of them.</li>
            </ul>
        </aside>
    </section>


    <section class="no_bg" style="top: 79.5px; display: block;">
        <h2>Without Containerization</h2>

        <img src='src/modules/Training-for-Containerization/02-docker-story/images/pre-container.png' width="80%"></img>

        <aside class='notes'>
            <ul>
                <li>Without containerization, we can imagine a host that looks as such. The key feature of this diagram is that all the dependencies, configurations, and system resources for applications on this host are shared.</li>
                <li>From the developer's standpoint, an uncontainerized environment requires strict discipline to avoid tight coupling between components; it's easy to develop your way into a state where changes to one component break another, and all components are affected by the environmental requirements of the others, complicating and hindering testing and development.</li>
                <li>For a devops engineer, the shared host environment of these components also adds friction. Every environment in the CI pipeline has to reflect this stack of dependencies and configurations, which can be difficult to maintain and difficult to reproduce.</li>
                <li>For operations personnel, the more tightly coupled different components become, the more difficult they can be to scale and monitor; tight couplings can make it complicated to understand what's needed to relax a performance bottleneck or trace root causes of application failures.</li>
            </ul>
        </aside>
    </section>

    <section class="no_bg">
        <h2>With Containerization</h2>

        <img src='src/modules/Training-for-Containerization/02-docker-story/images/post-container.png' width="80%"></img>

        <aside class='notes'>
            <ul>
                <li>The fundamental insight of the containerization movement was similar to that of the real-life logistics chains from which containerization got its name: many problems are solved by strictly encapsulating software as we develop, migrate, and deploy it.</li>
                <li>The key feature of a software container is that it is self-contained, all the way down to the OS filesystem; not only our executables, but all their configs and all their dependencies, and even the OS filesystem they run in, are captured in a container that can be moved as a complete unit.</li>
                <li>Modern Linux and Windows kernels can even represent the host system differently to different containers, presenting them different network devices, process trees, filesystems and more; all you need installed on the host is the Docker engine.</li>
            </ul>
        </aside>
    </section>

    <section class="no_bg">
        <h2>Rapid Development</h2>

        <img src='src/modules/Training-for-Containerization/02-docker-story/images/component-upgrade.png'></img>
        <p>Containers can be removed and replaced with a minimum of impact on their neighbors, increasing developer choice and speed.</p>

        <aside class='notes'>
            <ul>
                <li>From the developer's point of view, aggressive encapsulation equals speed and flexibility; since containers provide assurances that one component doesn't affect another, developers can upgrade one component while remaining confident they won't break others.</li>
                <li>Remember that containers carry the full dependency stack of the application they're designed to run, all the way down to the OS filesystem; this means that developers can use different stacks for different components, all on the same host with no chance of dependency conflicts.</li>
            </ul>
        </aside>
    </section>

    <section class="no_bg">
        <h2>Smooth Migration</h2>

        <img src='src/modules/Training-for-Containerization/02-docker-story/images/migration.png'></img>
        <p>Containers carry their environment and dependencies with them, simplifying and minimizing requirements on the hosts that run them.</p>

        <aside class='notes'>
            <ul>
                <li>From the devops point of view, aggressive encapsulation equals smooth migrations; since containers carry everything they need with them, they impose relatively few requirements on the host that runs them. Hosts need only provide a compatible kernel and architecture, and a container engine; no application specific dependencies or environment configurations need be maintained across environments.</li>
                <li>In this way, not only will a container that runs in dev likely run in prod, it will run the same way; containerized software passing tests in one environment has a better chance of passing tests in all environments, since environment matters comparatively little once we containerize our applications.</li>
            </ul>
        </aside>
    </section>

    <section class="no_bg">
        <h2>Simple Scale &amp; Maintenance</h2>

        <img src='src/modules/Training-for-Containerization/02-docker-story/images/scalability.png'></img>
        <p>Weak coupling between containers minimizes side effects when scaling and simplifies monitoring.</p>

        <aside class='notes'>
            <ul>
                <li>From the point of view of operations personnel, the encapsulation provided by containers translates into simple methodology for scaling and maintaining applications.</li>
                <li>Scale is achieved by containers in part by the minimal requirements they put on their environment; since containers are designed to run the same way regardless of their hosting environment, multiple instances can be spun up in parallel, or distributed across all the hosts in a datacenter.</li>
                <li>Furthermore, a well-designed container should run only a very limited set of tasks; by avoiding tight coupling and defining relatively simple behaviors for each container, it becomes simpler to define what 'healthy' means for a container, compared to a complete host with many interacting processes.</li>
            </ul>
        </aside>
    </section>

    <section class="no_bg">
        <h2>Secure by Default</h2>

        <img src='src/modules/Training-for-Containerization/02-docker-story/images/isolation.png'></img>
        <p>Containers have private system resources, so a compromise in one does not affect the rest.</p>

        <aside class='notes'>
            <ul>
                <li>From the point of view of security, aggressive encapsulation automatically enhances our security posture.</li>
                <li>On an uncontainerized host, if an attacker successfully compromises one component with elevated privileges, they can leverage those privileges across the entire host, using any vulnerable component as a point of ingress.</li>
                <li>On the other hand, containerized software mitigates its own vulnerabilities; even if an attacker compromises one container via a vulnerable component, privileges gained there do not grant the same level of access to other containers or to the host; root in a container does not equal root on the host.</li>
            </ul>
        </aside>
    </section>

    <section class="no_bg">
        <h2>Application Density</h2>

        <img src='src/modules/Training-for-Containerization/02-docker-story/images/density.png'></img>
        <p>Containers save datacenter costs by running many more application instances than virtual machines can on the same physical hosts.</p>

        <aside class="notes">
            <ul>
                <li>In terms of operational costs, containers can radically reduce the amount of metal needed to run a given workload compared to a virtual machine, because they are so resource efficient; containerized processes use the host kernel and don't require fixed allocations of CPU and memory, so can be run with the absolute minimal footprint.</li>
                <li>It's not unusual for the same datacenter to be able to run 10x as many containers encapsulating a given process, compared to VMs encapsulating the same software, resulting in substantial cost savings to users.</li>
            </ul>
        </aside>
    </section>

    <!--
    <section class="no_bg">
        <h2>The Containerization Stack</h2>

        <div style='text-align:center !important'>
            <img style='width:60%' src='src/modules/Training-for-Containerization/02-docker-story/images/container-stack.png'></img>
        </div>

        <aside class='notes'>
            <ul>
                <li>Any containerization stack needs three elements:</li>
                <li>A container runtime to actually make containers</li>
                <li>An orchestration layer to network containers together, potentially across remote hosts</li>
                <li>An enterprise tooling layer that provides for the management and security needs of large enterprises.</li>
                <li>Docker Community Edition provides the container runtime, and an orchestrator (based on SwarmKit), free for anyone to use. Kubernetes is also available as an alternative, powerful orchestration layer.</li>
                <li>Docker Enterprise Edition goes beyond the free product to provide tools like a secure API, role based access control, and tooling for creating container-based CI/CD pipelines.</li>
                <li>In this course, we're going to study the first two layers of the containerization stack; join us for our more advanced courses that dive into our enterprise tooling.</li>
            </ul>
        </aside>
    </section>
    -->
</section>
<section>
    <section class="no_bg">
        <h2>Containerization Basics</h2>
    </section>

    <section class="no_bg">
        <h2>Discussion: Running Containers</h2>

        <p>What assurances would you need to run a process on an arbitrary host? Consider</p>
        <ul>
            <li>Hostile environments</li>
            <li>Required resources</li>
        </ul>

        <aside class='notes'>
            <ul>
                <li>Guide the class to thinking about the need for process isolation (via kernel namespaces etc), filesystem provisioning (via Docker images), and resource consumption limitations (via cgroups)</li>
                <li>Hint questions if the class is stuck:</li>
                <li>Do you trust the other processes running on this host? Should they trust you? (leads to need for process isolation)</li>
                <li>Will your process' dependencies be available on this host? Are you sure? (leads to need for docker images)</li>
                <li>How much resources can you reasonably consume on this host? (leads to need for cgroup limitations)</li>
            </ul>
        </aside>

    </section>

    <section class="no_bg">
        <h2>Learning Objectives</h2>

        <p>By the end of this module, learners will be able to</p>
        <ul>
            <li>Describe what a container is in terms of processes and isolation tools</li>
            <li>Use the key commands for interacting with Docker containers</li>
        </ul>
    </section>

    <section class="no_bg">
        <h2>Containers are Processes</h2>

        <p><span class='keyword'>Containers</span> are processes sandboxed by</p>
        <ul>
            <li>Kernel namespaces</li>
            <li>Control Groups</li>
            <li>Root privilege management &amp; syscall restrictions (Linux)</li>
            <li>VM isolation (Windows)</li>
        </ul>

        <aside class='notes'>
            <ul>
                <li>Fundamentally, containers are just processes living on the host, isolated primarily by kernel namespaces, and control groups for resource isolation.</li>
                <li>Linux containers can also be subject to restrictions on what root privileges and system calls they are allowed to make, by application of a linux security module.</li>
                <li>On the Windows side, containers enhance their isolation by running in ultra-light-weight VMs.</li>
            </ul>
        </aside>
    </section>

    <!--
        <section class="no_bg">
        <h2>Architecture in Linux</h2>
        <div class="col-6">
            <img src="src/modules/Training-for-Containerization/03-container-basics/images/architecture-linux.png" title="Linux Architecture" class="transparent">
        </div>
    -->
    <!--
            <div class="col-6">
                <p>Architecture in Windows</p>
                <img src="src/modules/Training-for-Containerization/03-container-basics/images/architecture-windows.png" title="Windows Architecture" class="transparent">
            </div>
    --> 
    <!--
        </div>   <aside class="notes">
            <ul>
                <li>On a high level the architectures of Linux and Windows are not really that much different. Both rely on fundamental kernel features for creating containers like kernel namespaces and control groups.</li>
                <li>On the Linux side, a container runtime such as containerd uses these kernel features to create and run containers.</li>
                <li>On the Windows side, Microsoft has introduced the Compute Service to abstract the low level details of the capabilities like control groups, namespaces and layer capabilities and not make them (yet) public so that the Windows team can quickly iterate on those features without breaking any external contracts. In this regard Compute Service is the public interface the Docker platform communicates with.</li>
                <li>As said, this is a public API and thus not only Docker is using it but there exists multiple bindings, e.g. for C# and for Go (which is used by Docker).</li>
                <li>C#: https://github.com/Microsoft/dotnet-computervirtualization</li>
                <li>Go: https://github.com/Microsoft/hcsshim</li>
            </ul>
        </aside>
    </section>
    --> 

    <section class="no_bg">
        <h2>Linux Kernel Namespaces</h2>

        <ul>
            <li><b>DEFAULT</b>
                <ul>
                    <li>Process IDs</li>
                    <li>Network stacks</li>
                    <li>Inter-process communications</li>
                    <li>Mount points</li>
                    <li>Hostnames</li>
                </ul>
            </li><br>
            <li><b>OPTIONAL</b>
                <ul>
                    <li>User IDs</li>
                </ul>
            </li>
        </ul>

        <aside class="notes">
            <ul>
                <li>The baseline tool for creating containers is the kernel namespace. Kernel namespaces create distinct representations of things like PID trees, user spectra, network stacks and mount points; processes live in exactly one namespace, and are only able to interact with the broader system via the representation encapsulated therein.</li>
                <li>By analogy: an un-namespaced system is like when airplanes used to have just one TV in the cabin everyone would look up at to watch the in-flight movie. Everyone shared the device, and everyone saw the same thing.</li>
                <li>Introducing namespaces is like putting seatback TVs in front of every passenger. Now everyone has their own private devices and sees their own thing, which is hidden from their neighbors.</li>
                <li>In the same way, a namespaced process has all their own resources - their own iptables rules and eth0 device, their own mount points, their own PID tree - and processes in other namespaces aren't allowed to touch or even see these resources.</li>
            </ul>
        </aside>

    </section>

    <section class="no_bg">
        <h2>Linux PID Kernel Namespace</h2>

        <img src='src/modules/Training-for-Containerization/03-container-basics/images/pid-tree.png' style='width:70%'></img>

        <aside class="notes">
            <ul>
                <li>For example, PID namespaces make the first process in the namespace appear as the root of a process tree to all other processes in that namespace, which will be its children.</li>
                <li>Meanwhile, processes in the parent namespace see these processes with PID numbers like any other process in the parent PID namespace.</li>
                <li>In this way, processes in the child namespace aren't able to find information about processes in the parent namespace, but the child namespace remains transparent from the perspective of the parent namespace.</li>
                <li>Stopping the PID 1 of a child namespace and stopping the container are the exact same thing.</li>
                <li>Isolating host system resources, rather than creating a whole new virtual machine, is where the high performace of containers comes from. Think of it like building a little wall around a patch of sand in a sandbox; the area marked off can itself be thought of as a new sandbox, but no new sand has been acquired.</li>
            </ul>
        </aside>
    </section>

    <section class="no_bg">
        <h2>Optional Linux Isolation Features</h2>

        <ul>
            <li>Control groups: limit memory &amp; CPU</li>
            <li>Root privilege management: whitelist root powers</li>
            <li>System call management: whitelist available system calls</li>
            <li>Linux Security Modules: mandatory filesystem access control</li>
        </ul>

        <aside class='notes'>
            <li>In addition to the default kernel namespaces, a number of other linux technologies can be imposed on your containers to restrict their privileges.</li>
            <li>All production deployments should take advantage of control groups, to limit how much memory and CPU a container can consume.</li>
            <li>Root privileges and system calls are governed by whitelist, which can be configured per-container</li>
            <li>Linux security modules allow the creation of access control rules for filesystem paths and objects within the container.</li>
        </aside>

    </section>    

    <!--
    <section class="no_bg">
        <h2>Windows: Host Kernel Containers</h2>
        <img src="src/modules/Training-for-Containerization/03-container-basics/images/windows-containers.png" style='width:70%' title="Windows Containers" class="transparent">
        <aside class="notes">
            <ul>
                <li>Windows Containers can share the same Windows Kernel of the underlying Windows OS (either Windows Server 2016 or Windows 10).</li>
                <li>A fundamental difference between Linux and Windows containers is the infrastructure they require to make low-level calls to the kernel. In Linux, a container will have a single application process, which can simply issue system calls. But in Windows, a small collection of system processes are required alongside the application process to provide the same functionality; just as these system processes run in uncontainerized user mode, they must also be available inside a windows container.</li>
                <li>The implementation details of Windows containers are fundamentally different than those of linux containers, but they mimic the functionality and isolation provided by kernel namespaces and control groups.</li>
                <li>Microsoft has done and continues to do a lot of tremendous work to minimize this overhead, particularly with Nanoserver.</li>
            </ul>
        </aside>
    </section>

    <section class="no_bg">
        <h2>Windows: Hyper-V Containers</h2>
        <img src="src/modules/Training-for-Containerization/03-container-basics/images/hyper-v-containers.png" style='width:70%' title="Hyper-V Containers" class="transparent">
        <aside class="notes">
            <ul>
                <li>Windows containers alone don't have a set of isolation tools analogous to root privilege management or linux security modules.</li>
                <li>This has led to Hyper-V Containers. Hyper-v containers are Windows Containers that are wrapped inside a very special and slim VM. This means that from a developers or DevOps perspective nothing changes. These containers are built the exact same way as normal Windows containers and it is merely a switch in the `docker container run` command which determines in which mode the container should run.</li>
                <li>Hyper-V Containers provide all the security and isolation to a single container that we are used to when using VMs to isolate our applications. Yet due to special optimizations Hyper-V containers are much leaner and faster than traditional VMs and thus can be packed much more densely on a server.</li>
                <li>Hyper-V containers have one workload and deliver added isolation for multi-tenant or hostile environments</li>
                <li>The isolation provided by hyper-v containers provides a Windows alternative to the added isolation provided by linux security modules and root privilege management on the Linux side.</li>
            </ul>
        </aside>
    </section>
    -->
    
    <section data-background="#340B65" class="green_bg">
        <h2><img src="src/modules/Training-for-Containerization/03-container-basics/images/icon_task.png" class="moby_icon" alt="icon">Instructor Demo: Process Isolation</h2>
        
        <p>See the demo</p> 
        
        <ul>
            <li class='demo' script='process-isolation-demo.md'>Process Isolation</li>
        </ul>

        <p>In the Exercises book.</p>
        
    </section> 

    <section data-background="#00a2a1" class="green_bg">
        <h2><img src="src/modules/Training-for-Containerization/03-container-basics/images/icon_task.png" class="moby_icon" alt="icon"> Exercise: Container Basics</h2>
        <p>Work through</p> 
        <ul>
            <li class='exercise' script='running-and-inspecting-containers.md'>Running and Inspecting a Container</li>
            <li class='exercise' script='interactive-containers.md'>Interactive Containers</li>
            <li class='exercise' script='detached-containers-and-logging.md'>Detached Containers and Logging</li>
            <li class='exercise' script='starting-stopping-inspecting-and-deleting-containers.md'>Starting, Stopping, Inspecting and Deleting Containers</li>
        </ul>

        <p>In the Exercises book.</p>

        <h2 class="timer"></h2>
        
    </section>

    <section class="no_bg">
        <h2>Container Lifecycle</h2>

        <img src='src/modules/Training-for-Containerization/03-container-basics/images/container-lifecycle.png' style='width:80%'></img>

        <aside class="notes">
            <ul>
                <li>The rectangles display the state of the container and the arrow labels show the Docker command used to change the container state.</li>
                <li>The container lifecycle always begins in the CREATED state. A container in this state has a private filesystem set up on disk (more on this in the next chapter) and metadata defined regarding what process it is to encapsulate and how, but it not yet running. When the process in question begins running, the container transitions to the UP state.</li>
                <li>If a containerized process exits, the container transitions to the EXITED state. It can normally be restarted with a start command.</li>
                <li>Finally, Docker containers can enter a PAUSED state of suspension imposed by control group freezing. This suspension technique (unlike using SIGSTOP and SIGCONT) can't be caught by the process, ensuring that pausing a container doesn't disrupt the process it containerizes.</li>
            </ul>
        </aside>
    </section>

    <section class="no_bg">
        <h2>Container Logs</h2>

        <ul>
            <li>STDOUT and STDERR for a containerized process</li>
            <li><code>docker container logs &lt;container name&gt;</code></li>
        </ul>

        <aside class='notes'>
            <ul>
                <li>STDOUT and STDERR of whatever process is PID 1 in a container are logged by Docker, and available via `docker container logs`</li>
            </ul>
            
        </aside>
    </section>

    <section class="no_bg">
        <h2>Container Basics Takeaways</h2>
        <ul>
            <li>Single process constrained by kernel namespaces, control groups and other technologies</li>
            <li>Private &amp; ephemeral filesystem and data</li>
        </ul>
        <aside class="notes">
            <ul>
                <li>The key conceptual take-aways for container basics are the first two points; the container's main process, which it labels PID 1, will stop the container when it itself stops; and writing to the container's file system writes only to that container, not the underlying image; soon we'll learn more about these underlying images, and how to manipulate them.</li>
                <li>Everything else we learned in this unit is basic creation, deletion and investigation syntax.</li>
            </ul>
        </aside>
    </section>

    <section class="no_bg">
        <h2>Further Reading</h2>

        <ul>
            <li>List of container commands: <a href="http://dockr.ly/2iLBV2I">http://dockr.ly/2iLBV2I</a></li>
            <li>Getting started with containers: <a href="http://dockr.ly/2gmxKWB">http://dockr.ly/2gmxKWB</a></li>
            <li>Start containers automatically: <a href="http://dockr.ly/2xB8sMl">http://dockr.ly/2xB8sMl</a></li>
            <li>Limit a container's resources: <a href="http://dockr.ly/2wqN5Nn">http://dockr.ly/2wqN5Nn</a></li>
            <li>Keep containers alive during daemon downtime: <a href="http://dockr.ly/2emLwb5">http://dockr.ly/2emLwb5</a></li>
            <li>Isolate containers with a user namespace: <a href="http://dockr.ly/2gmyKdf">http://dockr.ly/2gmyKdf</a></li>
            <li>Intro to Windows Containers: <a href="https://dockr.ly/2CTYhYb">https://dockr.ly/2CTYhYb</a></li>
        </ul>

        <aside class='notes'>
            <ul>
                <li>additional resources about containers</li>
            </ul>
            
        </aside>
    </section>
</section>
<section>
    <section class="no_bg">
        <h2>Creating Images</h2>

        <aside class="notes">
            <ul>
                <li>In our discussion of containers, we focused on how processes are isolated.</li>
                <li>But if a process is to be truly portable, it also needs its filesystem and dependencies to come along with it.</li>
                <li>In this module, we'll explore images in-depth, including a focus on creating and modifying images.</li>
            </ul>
        </aside>
    </section>

    <section class="no_bg">
        <h2>Discussion: Provisioning Filesystems</h2>

        <p>What are some potential difficulties with provisioning entire filesystems for containers? How can we avoid these problems?</p>

        <aside class='notes'>
            <ul>
                <li>Guide the class to thinking about some of the non-obvious nuances around building and sharing images</li>
                <li>Obvious answer: disk and bandwidth usage</li>
                <li>Hint questions if the class is stuck:</li>
                <li>Are all container filesystems necessarily unique? If two containers share a filesystem, how will they remain independent? (leads to layer sharing and read-only images)</li>
                <li>What's a simple way to minimize the risk of vulnerable components making it onto your production servers via a Docker image? (answer: don't install any component you don't absolutely need, indicates the logic of minimal images)</li>
                <li>If containers and their filesystems are meant to move across environments like dev, testing, staging and prod without needing to be modified, how can they reflect the important differences between those environments? (leads to motivating multi-stage builds)</li>
            </ul>
        </aside>

    </section>

    <section class="no_bg">
        <h2>Learning Objectives</h2>

        <p>By the end of this module, learners will be able to</p>
        <ul>
            <li>Create images via several methods</li>
            <li>Describe the filesystem structure underlying an image</li>
            <li>Understand the performance implications of different image design decisions</li>
            <li>Correctly tag and namespace images for distribution on a registry</li>
        </ul>
    </section>

    <section class="no_bg">
        <h2>What are Images?</h2>

        <div class='row'>
            <div class='col-6'>
                <ul>
                    <li>A <span class="keyword">filesystem</span> for container process</li>
                    <li>Made of a stack of <span class="keyword">immutable</span> layers</li>
                    <li>Start with a <span class="keyword">base image</span></li>
                    <li>New layer for each change</li>
                </ul>
            </div>
            <div class='col-6' style='text-align:right'>
                <img src='src/modules/Training-for-Containerization/04-creating-images/images/lfs-1.png'></img>
            </div>            
        </div>

        <aside class="notes">
            <ul>
                <li>Images are composed in layers; each layer consists of a bunch of files that capture how this layer adapts the one beneath it.</li>
                <li>Note: On Windows there are also Windows registy entries that are captured as part of the layer.</li>
                <li>These stacks of layers always start with a base image, which typically captures only the base operating system for this image.</li>
                <li>each subsequent image layer captures sequential changes to the image.</li>
            </ul>
        </aside>
    </section>

    <section class="no_bg">
        <h2>Sharing Layers</h2>

        <img src='src/modules/Training-for-Containerization/04-creating-images/images/lfs-2.png'></img>

        <aside class="notes">
            <p>benefits of layering: (see if students can guess):</p>
            <ul>
                <li>Sharing layers == smaller on disk and in memory</li>
                <li>Sharing layers == faster downloads (de-duped by default)</li>
                <li>Allows caching when constructing images</li>
            </ul>
        </aside>
    </section>

    <section class="no_bg">
        <h2>The Writable Container Layer</h2>

        <img src='src/modules/Training-for-Containerization/04-creating-images/images/lfs-3.png'></img>

        <aside class="notes">
            <ul>
                <li>Starting a container essentially adds a single writable layer to the image stack; since Docker is just adding this one thin layer, container startup is very fast and resource efficient.</li>
                <li>Any manipulations of the filesystem a container does is written only to this R/W layer; all image layers are always read-only.</li>
                <li>When a container edits a file from the base image, then and only then is that file copied to the R/W layer; this is what is meant by Docker's 'copy on write' filesystem; this also implies that the copy of a file that is visible in a running container is whichever copy of that file sits highest in the stack of filesystem layers.</li>
            </ul>
        </aside>
    </section>

    <section class="no_bg">
        <h2>Images: Copy on Write</h2>

        <img src='src/modules/Training-for-Containerization/04-creating-images/images/copy-on-write.png' style='width:80%'></img>

        <aside class='notes'>
            <ul>
                <li>The final product is composed per this diagram, via what we call a 'copy on write' composition strategy</li>
                <li>Each time a layer is added, only files that are changed are copied up to the next layer; each of these layers actually exists as a directory on your host machine.</li>
            </ul>
        </aside>
    </section>
    
    <section class="no_bg">
        <h2>Linux Containers: Union FS</h2>

        <img src='src/modules/Training-for-Containerization/04-creating-images/images/linux-ufs.png' style='width:70%'></img>

        <aside class='notes'>
            <ul>
                <li>When creating a container on Linux, a R/W container layer is created, and all these filesystem layers are composed via a union filesystem mount. This assembles the image layers into a unified filesystem similar to superimposing a stack of overhead transparencies on top of each other; files on higher layers obscure earlier versions of themselves on lower layers.</li>
                <li>When a container modifies a file from the image, it performs the same copy on write action as above, into the R/W container layer.</li>
            </ul>
        </aside>

    </section>

    <!--
    <section class="no_bg">
        <h2>Windows Containers: Linked FS</h2>

        <img src='src/modules/Training-for-Containerization/04-creating-images/images/windows-fs.png' style='width:70%'></img>

        <aside class='notes'>
            <ul>
                <li>On the Windows side, making union file systems backwards compatible with NTFS wasn't feasible.</li>
                <li>Instead, Windows uses reparse points to effectively link the appropriate image files into a container layer; if a file is unmodified in the layer, a stub is created for it containing the reparse point. Actually, each image layer also contains reparse points to earlier reparse points, in a chain down to the original file.</li>
                <li>When a Windows container changes a file in an image, it's copied to the virtual hard drive where the changes can be made.</li>
            </ul>
        </aside>
    </section>
    -->
    
    <section class="no_bg">
        <h2>Creating Images</h2>
        <p>Three methods:</p>
        <ul>
            <li><span class="keyword">Commit</span> the R/W container layer as a new R/O image layer.</li>
            <li>Define new layers to add to a starting image in a <span class="keyword">Dockerfile</span>.</li>
            <li><span class="keyword">Import</span> a tarball into Docker as a standalone base layer.</li>
        </ul>
    </section>

    <section class="no_bg">
        <h2>Committing Container Changes</h2>
        <ul>
            <li><code>docker container commit</code><br>saves container layer as new R/O image layer</li>
            <li>Pro: build images interactively</li>
            <li>Con: hard to reproduce or audit; <span class='keyword'>avoid this</span> in practice.</li>
        </ul>

        <aside class='notes'>
            <ul>
                <li>One way of building up images is to save the container layer as a new image layer</li>
                <li>This is fine for experiments, but it's really something best avoided in the development of production grade code, since it isn't easily auditable, reproducible or automated.</li>
            </ul>
        </aside>
    </section>

    <section class="no_bg">
        <h2>Dockerfiles</h2>
        <ul>
            <li>Content manifest</li>
            <li>Provides image layer documentation</li>
            <li>Enables automation (CI/CD)</li>
        </ul>

        <aside class='notes'>
            <ul>
                <li>Interactive image creation is good for tinkering, but its main drawback is that it doesn't produce an artifact describing the steps to create the image in a machine-readable way.</li>
                <li>Therefore, there's no way to build images this way as part of a CI/CD chain, and it can be hard to audit what exactly is in the image.</li>
                <li>A Dockerfile is essentially a recipe to build an image, layer by layer. This can be ingested in build processes and CI/CD pipelines, and preserves a record of all the steps taken to create an image.</li>
            </ul>
        </aside>
    </section>

    <section class="no_bg">
        <h2>Dockerfiles</h2>
        <ul>
            <li><code>FROM</code> command defines base image.</li>
            <li>Each subsequent command adds a layer or metadata</li>
            <li><code>docker image build ...</code> builds image from Dockerfile</li>
        </ul>
        <br><br>
        <div class="row">
            <div class="col-6">
                <div class='pre'># Comments begin with the pound sign
<span class="red-bg">FROM</span> ubuntu:16.04
<span class="red-bg">RUN</span> apt-get update &amp;&amp; apt-get install -y wget
<span class="red-bg">ADD</span> /data /myapp/data
...</div>
            </div>
            <!--
            <div class="col-6">
                Windows containers:<br>
                <div class="pre"># Comments begin with the pound sign
<span class="red-bg">FROM</span> microsoft/nanoserver:latest
<span class="red-bg">RUN</span> Install-Module -Name Nuget -Force
<span class="red-bg">ADD</span> c:\\myapp\data c:\\app\data
...</div>
            </div>
        -->
        </div>

        <aside class='notes'>
            <ul>
                <li>Note that dockerfiles for linux and windows are syntactically identical; they use different images for their bases and run different processes at each step, but the way we specify our image recipe doesn't change at all.</li>
            </ul>
        </aside>
    </section>

    <section  data-background="#00a2a1" class="green_bg">
        <h2><img src="src/modules/Training-for-Containerization/04-creating-images/images/icon_task.png" class="moby_icon" alt="icon"> Exercises: Creating Images</h2>
        <p>Work through</p> 
        <ul>
            <li class='exercise' script='interactive-image-creation.md'>Interactive Image Creation</li>
            <li class='exercise' script='creating-images-with-dockerfiles-part-1.md'>Creating Images with Dockerfiles (1/2)</li>
        </ul>
        <p>in the Exercises book.<p>
        <h2 class="timer"></h2>
    </section> 

    <section data-background="#340B65" class="green_bg">
        <h2><img src="src/modules/Training-for-Containerization/04-creating-images/images/icon_task.png" class="moby_icon" alt="icon"> Instructor Demo: Creating Images</h2>

        <p>See the demo</p> 
        
        <ul>
            <li class='demo' script='creating-images-demo.md'>Creating Images</li>
        </ul>

        <p>In the Exercises book.</p>
    </section> 
    
    <section class="no_bg">
        <h2>Build Cache</h2>
        <div class='row'> 
            <div class='col-4'>
                <img src='src/modules/Training-for-Containerization/04-creating-images/images/building-images.png' style='width:80%'></img>
            </div>
            <div class='col-8'>
                <ul>
                    <li>After completion, the resulting image layer is labeled with a hash of the content of all current image layers in the stack.</li>
                </ul>
            </div>
        </div>

        <aside class='notes'>
            <ul>
                <li>Layers are fetched from the cache via the hash label affixed to that layer the first time it was created.</li>
                <li>Q: Why is a hash for a layer computed based on the entire image? Why not just that layer?</li>
                <li>A: A layer can't be reused unless all layers under it are the same; put another way, the effect of whatever command generated the layer might be different depending on substrate layers.</li>
                <li>The upshot being that the builder will stop using the cache at the first change in the Dockerfile.</li>
            </ul>
        </aside>
    </section>

    <section class="no_bg">
        <h2>CMD and ENTRYPOINT</h2>
        <ul>
            <li>Recall all containers run a process as their PID 1</li>
            <li><code>CMD</code> and <code>ENTRYPOINT</code> allow us to specify default processes.</li>
        </ul>

        <aside class='notes'>
            <ul>
                <li>Another pair of helpful commands in Dockerfiles are CMD and ENTRYPOINT</li>
                <li>These are used for specifying default processes and options to run in containers created from this image.</li>
            </ul>
        </aside>
    </section>

    <section class="no_bg">
        <h2>CMD and ENTRYPOINT</h2>
        <ul>
            <li><code>CMD</code> alone: default command and list of parameters.</li>
            <li><code>CMD</code> + <code>ENTRYPOINT</code>: <code>ENTRYPOINT</code> provides command, <code>CMD</code> provides default parameters.</li>
            <li><code>CMD</code> overridden by command arguments to <code>docker container run</code>
            <li><code>ENTRYPOINT</code> overridden via <br><code>--entrypoint</code> flag to <code>docker container run</code>.</li>
        </ul>

        <aside class='notes'>
            <ul>
                <li>Oftentimes images are designed to do exactly one thing; CMD and ENTRYPOINT allow you to bake that intention right into the image, by pre-specifying that command.</li>
                <li>The difference between the two is essentially in how you want to override these defaults</li>
                <li>Using them together makes your container feel a lot like an executable; arguments (defaulted by CMD) will be overridden by command line args, but the executable defined by ENTRYPOINT will not.</li>
            </ul>
        </aside>
    </section>

    <section class="no_bg">
        <h2>Shell vs. exec format</h2>
        <div class="row">
            <div class="col-6">
                <div class='pre'># Shell form
CMD sudo -u ${USER} java ...

# Exec form
CMD ["sudo", "-u", "jdoe", "java", ...]</div>
            </div>
            <!--
            <div class="col-6">
                Windows containers:<br>
                <div class='pre'># shell form
CMD c:\\Apache24\\bin\\httpd.exe -w

# exec form
CMD ["c:\\Apache24\\bin\\httpd.exe", "-w"]</div>
                <p>Note the "\\" in the expressions</p>
            </div>
            -->
        </div>
        <aside class='notes'>
            <ul>
                <li>CMD, ENTRYPOINT and RUN commands can use either exec or shell syntax</li>
                <li>If we have a command like this on Windows `powershell New-Item c:\test` then if it is in declared in shell form what is executed is in reality `cmd /S /C powershell New-Item c:\test` whilst in exec form the command is executed as is without the use of the shell (cmd in this case). The analogous is true for Linux containers.</li>
                <li>exec is generally preferred for ENTRYPOINT, since it preserves the ability to override options.</li>
                <li>
                    subtle differences:
                    <ul>
                        <li><i>Shell form</i> allows for the parsing of variables like <code>CMD sudo -u ${USER} java ... </code></li>
                        <li><i>Exec form</i> can run in a container with no shell; shell form always runs via <code>/bin/sh -c</code></li>
                        <li><i>Shell form</i> for <code>ENTRYPOINT</code> prevents options from being overridden by <code>CMD</code> or <code>docker container run</code>.</li> 
                    </ul>
                </li>
                <li>Note that exec form is formal JSON - double quotes mandatory.</li>
                <li>When using the shell form, the specified binary is executed with an invocation of the shell using /bin/sh -c, which means the process running as PID 1 is the /bin/sh executable.</li>
            </ul>
        </aside>
    </section>

    <section data-background="#00a2a1" class="green_bg">
        <h2><img src="src/modules/Training-for-Containerization/04-creating-images/images/icon_task.png" class="moby_icon" alt="icon"> Exercise: Dockerfiles (2/2)</h2>

        <p>Work through</p> 
        <ul>
            <li class='exercise' script='creating-images-with-dockerfiles-part-2.md'>Creating Images with Dockerfiles (2/2)</li>
        </ul>

        <p>In the Exercises book.</p>
        <h2 class="timer"></h2>
    </section>

    <section class="no_bg">
        <h2>COPY and ADD commands</h2>

        <p><code>COPY</code> copies files from build context to image</p>
        <pre class='large'>COPY &lt;src&gt; &lt;dest&gt;</pre>

        <div>
            <p><code>ADD</code> can also <span class='keyword'>untar</span>* or <span class='keyword'>fetch URLs</span>.</p>
            <div style="font-style: italic; font-size: 0.5em !important; color:darkgrey !important;">* Linux containers only!</div>
        </div>    
        <p>In both cases</p>
        <ul>
            <li>create checksum for files added</li>
            <li>log checksum in build cache</li>
            <li>cache invalidated if checksum changed</li>
        </ul>

        <aside class='notes'>
            <ul>
                <li>COPY and ADD add files from the local filesystem to the image</li>
                <li>Build process uses a checksum against the files to be added to bust the cache if those files have changed</li>
                <li>Note that ADD can also copy files from a URL and for Linux containers only(!) untar files upon copying them into the image.</li>
            </ul>
        </aside>
    </section>

    <section class="no_bg">
        <h2>Dockerfile Command Roundup</h2>

        <ul>
            <li><span class='keyword'>FROM</span>: base image to start from (usually OS)</li>
            <li><span class='keyword'>RUN</span>: run a command in the environment defined so far</li>
            <li><span class='keyword'>CMD</span> and <span class='keyword'>ENTRYPOINT</span>: define default behavior</li>
            <li><span class='keyword'>COPY</span> and <span class='keyword'>ADD</span>: copy files into container</li>
        </ul>

        <p>Many more Dockerfile commands are available; see the docs at <a href="https://docs.docker.com/engine/reference/builder/"><span ></span>https://docs.docker.com/engine/reference/builder/</span></a></p>

        <aside class='notes'>
            <ul>
                <li>We've seen the greatest hits of Dockerfile commands, but there are tons more; see the docs.</li>
            </ul>
        </aside>

    </section>

    <section class="no_bg">
        <h2>Advanced Dockerfile Construction</h2>

        <p>How can we build images that are</p>

        <ul>
            <li>Lightweight</li>
            <li>Secure</li>
            <li>Minimal build times</li>
        </ul>

        <aside class='notes'>
            <ul>
                <li>Now that we've seen the basics of image construction with Dockerfiles, we'd like to investigate best practices around image construction</li>
                <li>Our priorities for image creation are size, security, and build times.</li>
                <li>Size and security can be addressed by similar techniques; making sure we only install things we absolutely need in our image not only keeps the image size down, but avoids exposing ourselves to potential vulnerabilities in superfluous components.</li>
                <li>Also during the course of development, we'd like build times to be as fast as possible, either by leveraging the cache we've already seen, or by parallelizing parts of the build process.</li>
                <li>For the next part of this chapter, we'll look at some advanced techniques for achieving all of these.</li>
            </ul>
        </aside>

    </section>

    <section class="no_bg">
        <h2>The Scratch Image</h2>
        <ul>
            <li>An "empty" image</li>
            <li>Can't be pulled</li>
            <li>Doesn't create a layer</li>
            <li>Used for building images not based on any pre-existing image</li>
            <li>Linux only</li>
        </ul>

        <div class='pre'>FROM <span class="red-bg">scratch</span>

ADD centos-7-docker.tar.xz /

LABEL org.label-schema.schema-version="1.0" \
org.label-schema.name="CentOS Base Image" \
org.label-schema.vendor="CentOS" \
org.label-schema.license="GPLv2" \
org.label-schema.build-date="20181205"

CMD ["/bin/bash"]</div>

        <aside class='notes'>
            <ul>
                <li>The scratch image is an empty image that exists in Docker Hub, but has no tags and can't be pulled.</li>
                <li>When used in a Dockerfile, the line `FROM scratch` doesn't add any layer to the image. The next command in the Dockerfile will be the first filesystem layer.</li>
                <li>The scratch image is used typically to build base images with as few components as possible installed in them, to give the smallest possible attack surface to our images.</li>
            </ul>
        </aside>
    </section>

    <section class="no_bg">
        <h2>Multi-Stage Builds</h2>

        <div class="row">
            <div class="col-6">
                <div style="font-size: 0.5em;">Hello World, in C:</div>
                <div class="pre">FROM alpine:3.5
RUN apk update &amp;&amp; \
    apk add --update alpine-sdk
RUN mkdir /app
WORKDIR /app
ADD hello.c /app
RUN mkdir bin
RUN gcc -Wall hello.c -o bin/hello 
CMD /app/bin/hello</div>
            </div>
            <!--
            <div class="col-6">
                Windows containers:<br>
                <div style="font-size: 0.5em;">Hello World, in Go:</div>
                <div class="pre">FROM golang:nanoserver</span>
COPY . /code
WORKDIR /code
RUN go build hello.go
CMD ["\\code\\hello.exe"]</div>
            </div>
-->
        </div>
        <br>
        <div style="font-size: 0.6em;">Builds to:</div>

        <pre>
$ docker image ls hwc
REPOSITORY          TAG                 IMAGE ID            CREATED             SIZE
hwc                 latest              142c29686b6a        15 hours ago        184 MB</pre>

        <aside class='notes'>
            <ul>
                <li>Here's a Dockerization of hello world, in C. By now, we should recognize the steps: we start from an operating system, use RUN to install dependencies, ADD to import files from our host machine, and define some default behavior with CMD.</li>
                <li>There's just one problem: we have successfully made a hello world application in a mere 184 MB. Giant images are at best slow to start, and can have security problems depending on what unnecessary components have been included.</li>
                <li>Most of this bloat is due to things we don't actually need in production: compilers, developer tools and the like.</li>
                <li>The Docker image builder implements Multi Stage Builds to allow you to create executables, then throw away the scaffolding needed to compile them, leaving you with a fast, lightweight image.</li>
            </ul>
        </aside>
    </section>    

    <section class="no_bg">
        <h2>Multi-Stage Builds</h2>

        <p>Hello World, lightweight:</p>

        <div class="row">
            <div class="col-6">
                <div class="pre"># Full SDK version (built and discarded)
FROM alpine:3.5 <span class="red-bg">AS build</span>
RUN apk update &amp;&amp; \
    apk add --update alpine-sdk
RUN mkdir /app
WORKDIR /app
ADD hello.c /app
RUN mkdir bin
RUN gcc -Wall hello.c -o bin/hello 

# Lightweight image returned as final product
FROM alpine:3.5
<span class="red-bg">COPY --from=build /app/bin/hello /app/hello</span>
CMD /app/hello</div>

            </div>
            <!--
            <div class="col-6">
                Windows containers:<br>
                <div class="pre">FROM golang:nanoserver <span class="red-bg">as gobuild</span>
COPY . /code
WORKDIR /code
RUN go build hello.go

FROM microsoft/nanoserver
<span class="red-bg">COPY --from=gobuild /code/hello.exe /hello.exe</span>
EXPOSE 8080
CMD ["\\hello.exe"]</div>
            </div>
    -->
        </div>
        <br>
        <div style="font-size: 0.6em;">Builds to:</div>
        <div class="pre">$ docker image ls hwc
REPOSITORY          TAG                 IMAGE ID            CREATED             SIZE
hwc                 latest              5d925cfc9c96        39 seconds ago      <span class="red-bg">4MB</span></div>

        <aside class='notes'>
            <ul>
                <li>To make a lightweight version of hello world with all the developer tools stripped out, we start with the exact same Dockerfile, but we've added the AS clause to the FROM statement.</li>
                <li>Then, we've added a second stanza, where we start from the same OS, but instead of installing the developer's kit, we use the --from flag with COPY to reference the 'build' image described above, and copy just the final executable over into our final image.</li>
                <li>The --from flag to COPY can also also specify an earlier image by index counting from 0 (so --from=0 would have had the same effect in the second stanza above).</li>
                <li>Note that it kind of looks like we built two images here - in fact, only the final FROM stanza results in an image on disk. All previous stanzas create cached image layers, but no final image.</li>
            </ul>
        </aside>        
    </section>

    <section class="no_bg">   
        <h2>Build Targets</h2>
        <p>Dockerfile</p>
        <div><pre>FROM &lt;base image&gt; as base
...

FROM &lt;foo image&gt; as foo
...

FROM &lt;bar image&gt; as bar
...

FROM alpine:3.4
...
COPY --from foo ...
COPY --from bar ...
...</pre></div>  
        <p>Building the image</p>
        <code>docker image build --target &lt;name&gt; ...</code>

        <aside class="notes">
            <ul>
                <li>We can also build intermediate images by specifying the "--target" parameter with the name of the intermediate build.</li>
                <li>If no "--target" is provided then the "docker image build" command always builds only the last image (the one starting with the last FROM statement in the Dockerfile)</li>
                <li>The &lt;name&gt; of an intermediate image is either the index of the FROM in the Dockerfile or the alias provided in the FROM statement (e.g. FROM base as test - in that case &lt;name&gt; would be "test")</li>
            </ul>
        </aside>
    </section>

   <!--
       <section class="no_bg">
        <h2>BuildKit</h2>

        <ul>
            <li>Speed-optimized builder, enable via <code>export DOCKER_BUILDKIT=1</code></li>
            <li>Parallelizes multi-stage builds</li>
            <li>Custom frontends</li>
            <li>2x - 9x build speedup</li>
            <li>Linux only as of 18.09.0-ee</li>
        </ul>

        <aside class='notes'>
            <ul>
                <li>BuildKit is an optional builder designed to make the image building process faster, better optimized, and more customizable.</li>
                <li>BuildKit parallelizes multistage builds and introduces a number of behind-the-scenes optimizations for creating a greater than 2x speedup in build times.</li>
                <li>BuildKit also supports the creation of custom frontends that allow new features and customizations to be more easily integrated into the build process.</li>
            </ul>
        </aside>

    </section>
   --> 

    <section data-background="#00a2a1" class="green_bg">
        <h2><img src="src/modules/Training-for-Containerization/04-creating-images/images/icon_task.png" class="moby_icon" alt="icon"> Exercise: Multi-Stage Builds</h2>

        <p>Work through</p> 
        <ul>
            <li class='exercise' script='multi-stage-builds.md'>Multi-Stage Builds</li>
        </ul>

        <p>In the Exercises book.</p>
        <h2 class="timer"></h2>
    </section>

    <section class="no_bg">
        <h2>Image Construction Best Practices</h2>

        <ul>
            <li>Start with official images</li>
            <li>Use multi-stage builds to drop compilers, SDKs...</li>
            <li>More layers leverage the cache...</li>
            <li>...but fewer layers perform better.</li>
        </ul>

        <aside class='notes'>
            <ul>
                <li>Now that we have the mechanics of making Dockerfiles, there's also a number of optional best practices to consider.</li>
                <li>Base your images off of official images whenever possible; you can recognize these on Docker Hub as they don't have an explicit namespace like vendor/product; they're just single-word names, possibly with a tag. These are all battle-tested images produced in collaboration between the product vendors and Docker, and are scanned regularly for security vulnerabilities.</li>
                <li>Take advantage of multi-stage builds; these allow you to drop unnecessary layers, which will result in faster container start times, and less components that potentially inject vulnerabilities into your containers.</li>
                <li>Deciding how many layers to build an image out of depends on your priorities. The fundamental tension is that more layers leverage the cache better (since hopefully you don't invalidate the cache until you're most of the way through your Dockerfile), but this creates more overhead at container runtime, which you may wish to avoid for production images.</li>
            </ul>
        </aside>
    </section>

    <section class="no_bg">
        <h2>Development: More Layers</h2>

        <div class='row'>
            <div class='col-6'>
                <p>Bad caching:</p>
                <pre class='large'>
FROM python:3.5-alpine
RUN mkdir /app
<span class="red-bg">COPY /mypy /app/</span>
RUN pip install -r app/reqs.txt
...</pre>
            </div>
            <div class='col-6'>
                <p>Good caching:</p>
                <pre class='large'>
FROM python:3.5-alpine
RUN mkdir /app
<span class="red-bg">COPY /mypy/reqs.txt /app/</span>
RUN pip install -r app/reqs.txt
<span class="red-bg">COPY /mypy /app/</span>
...</pre>
            </div>
        </div>

        <aside class='notes'>
            <ul>
                <li>A common best practice during development is to split up oft-changing and rarely-changing elements into different layers. Move the rarely-changing parts as high as possible in the Dockerfile, so they don't have to be redone when the frequently changing parts are changed.</li>
                <li>In this case, we save ourselves from redoing the `pip install` when anything other than the requirements file changes.</li>
            </ul>
        </aside>

    </section>

    <section class="no_bg">
        <h2>Production: Less Layers</h2>

        <ul>
            <li>
                To collapse ALL image layers:
                <pre class='large'>
docker container run -d --name demo mytallimage:1.0
docker container export demo > image.tar
cat image.tar | docker image import - myflatimage:1.0</pre>
            </li>
            <li>Or build with <code>--squash</code> flag (experimental): compress all non-base layers</li>
            <li>Combine <code>container export</code> with <code>--squash</code> for one shareable base layer + one application-specific upper layer</li>
        </ul>

        <aside class='notes'>
            <ul>
                <li>Once it's time to go to production (or even to start CI/CD), we don't care so much about build times and caching. The image is nominally built - what matters is performance.</li>
                <li>One way to compress everything into a single layer is to export a container as a tarball, and reimport it as a new, single layer image. This completely destroys the ability of containers to share layers, though</li>
                <li>Another method is the experimental squash flag, which combines all non-base layers into a single layer. Now the base layer remains sharable, and our production image is only two layers.</li>
                <li>One technique for getting the best of both worlds when layer sharing is important is to use the first method to collapse all widely shared layers into a common base image, and then use the --squash flag on subsequent builds to squash the application-unique layers into a single application layer.</li>
            </ul>
        </aside>

    </section>

    <section class="no_bg">
        <h2>Best Practice: Patching &amp; Updates</h2>

        <img src="src/modules/Training-for-Containerization/04-creating-images/images/patching.png" title="Patching and Updates" class="transparent" style='width:60%'>
        <aside class="notes">
            <ul>
                <li>When revving an image, don't just apply patches on top of old images. If it's your base layer that's been revved, the vendor will likely release a new image corresponding to the new software version; update your Dockerfile and rebuild your image with the new base layer.</li>
                <li>The same logic holds true for updating other image layers; rebuild your image from its Dockerfile, pulling in the desired versions of your dependencies, rather than just installing patches on top of patches like you would for software installed on the host.</li>
                <li>Remember copy on write: when you apply a patch, it doesn't overwrite whatever its upgrading; all versions of all files are persisted in their entirety in an ever-growing image layer stack. This will bloat your images and slow down their performance.</li>
            </ul>
        </aside>
     </section>

    <section class="no_bg">
        <h2>Image Tags</h2>

        <ul>
            <li>Optional string after image name, separated by <code>:</code></li>
            <li><code>:latest</code> by default</li>
            <li>Same image with two tags shares same ID, image layers:</li>
        </ul>

        <pre>
$ docker image ls centos*
REPOSITORY          TAG                 IMAGE ID            CREATED             SIZE
centos              7                   8140d0c64310        7 days ago          193 MB
$ docker image tag centos:7 centos:mytag
$ docker image ls centos*
REPOSITORY          TAG                 IMAGE ID            CREATED             SIZE
centos              7                   8140d0c64310        7 days ago          193 MB
centos              mytag               8140d0c64310        7 days ago          193 MB</pre>

        <aside class='notes'>
            <ul>
                <li>In addition to the name of the image, images can be given an optional tag.</li>
                <li>Tags are often used to capture version number or base image distro.</li>
                <li>The tag will default to `latest` if omitted.</li>
                <li>Note that tags are essentially just pointers to an image which is uniquely identified by its ID; creating another tag pointing to the same image doesn't duplicate the image on disk, but just creates another reference to it.</li>
            </ul>
        </aside>

    </section>

    <section class="no_bg">
        <h2>Image Namespaces</h2>
        <p>Images exist in one of three namespaces:</p>
        <ul>
            <li>Root (<code>ubuntu</code>, <code>nginx</code>, <code>mongo</code>, <code>mysql</code>, ...)</li>
            <li>
                User / Org (<code>jdoe/myapp:1.1</code>, 
                <code>microsoft/nanoserver:latest</code>, ...)
            </li>
            <li>Registry (<code>FQDN/jdoe/myapp:1.1</code>, ...)</li>
        </ul>

        <aside class="notes">
            <ul>
                <li>Certified images produced in collaboration between Docker and third-party software vendors are given single-word names in the root namespace.</li>
                <li>Images meant to be shared on hub.docker.com are namespaced via the owning account, then the image name</li>
                <li>Images stored in docker trusted registry are similar to hub.docker.com names, but prefixed with the FQDN of the registry.</li>
            </ul>
        </aside>
    </section>

    <section class="no_bg">
        <h2>Image Tagging &amp; Namespacing</h2>

        <ul>
            <li>Tag on build:<br>
                <div class="pre large">docker image build -t myapp<span class="red-bg">:1.0</span> .</div>
            </li>
            <li>Retag an existing image:<br>
                <div class="pre large">docker image tag myapp:1.0 <span class="red-bg">me/myapp:2.0</span></div>
            </li>
            <li>Note <code>docker image tag</code> can set both tag and namespace.</li>
            <li>Names and tags are just pointers to image ID</li>
            <li>Image ID corresponds to immutable content addressable storage</li>
        </ul>

        <aside class="notes">
            <ul>
                <li>Images can be tagged on build or retagged at any time.</li>
                <li>Note that image layers are only stored once per machine; retagging or renaming an image does not duplicate the image layer.</li>
                <li>Always remember that an image must be namespaced correctly to push to a registry, whether it's hub.docker.com or Docker Trusted Registry.</li>
                <li>Finally, remember that docker registries all use content addressable storage models; image names and tags are really just human-friendly pointers to image IDs, which serve as the true address for immutable image information. As such, it is a good security strategy to pull by sha and not by tag; then you always know exactly what you're getting.</li>
            </ul>
        </aside>

    </section>

    <section class="no_bg">
        <h2>Sharing Images</h2>

        <ul>
            <li>
                Docker Hub
                <ul>
                    <li>Provides certified commercial and free software distributed as Docker Images</li>
                    <li>Shares community-generated images and content</li>
                </ul>
            </li>
        </ul>

        <aside class="notes">
            <ul>
                <li>Docker Hub allows you to access and share your public repositories with the Docker community at large. You can download two types of images from the Docker Hub: Docker Verified Images and Community/Hub images.</li>
                <li>Docker Hub is a cloud-based registry service which allows you to link to code repositories, build your images and test them, stores manually pushed images, and links to Docker Cloud so you can deploy images to your hosts. If you have built images, you can push them to a Docker Hub repository that you add to your Docker Hub user or organization account.</li>
            </ul>
        </aside>
    </section>

    <section data-background="#00a2a1" class="green_bg">
        <h2><img src="src/modules/Training-for-Containerization/04-creating-images/images/icon_task.png" class="moby_icon" alt="icon"> Exercise: Managing Images</h2>

        <p>Work through</p> 
        <ul>
            <li class='exercise' script='managing-images.md'>Managing Images</li>
        </ul>

        <p>In the Exercises book.</p>
        <h2 class="timer"></h2>
    </section> 

    <section class="no_bg">
        <h2>Image Creation Takeaways</h2>

        <ul>
            <li>Images are built out of read-only layers.</li>
            <li>Dockerfiles specify image layer contents.</li>
            <li>Key Dockerfile commands: <code>FROM</code>, <code>RUN</code>, <code>COPY</code> and <code>ENTRYPOINT</code></li>
            <li>Images must be namespaced according to where you intend on sharing them.</li>
        </ul>

    </section>

    <section class="no_bg">
        <h2>Further Reading</h2>
        <ul>
            <li>Best practices for writing Dockerfiles: <a href="http://dockr.ly/22WiJiO">http://dockr.ly/22WiJiO</a></li>
            <li>Use multi-stage builds: <a href="http://dockr.ly/2ewcUY3">http://dockr.ly/2ewcUY3</a></li>
            <li>More about images, containers, and storage drivers: <a href="http://dockr.ly/1TuWndC">http://dockr.ly/1TuWndC</a></li>
            <li>Details on image layering: <a href='https://bit.ly/2AHX7iW'>https://bit.ly/2AHX7iW</a></li>
            <li>Graphdriver plugins: <a href="http://dockr.ly/2eIVCab">http://dockr.ly/2eIVCab</a></li>
            <li>Docker Reference Architecture: An Intro to Storage Solutions for Docker CaaS: <a href="http://dockr.ly/2x8sBw2">http://dockr.ly/2x8sBw2</a></li>
            <li>How to select a storage driver: <a href="http://dockr.ly/2eDu8yO">http://dockr.ly/2eDu8yO</a></li>
            <li>Use the AUFS storage driver: <a href="http://dockr.ly/2jVc1Zz">http://dockr.ly/2jVc1Zz</a></li>
            <li>User guided caching in Docker: <a href="http://dockr.ly/2xKafPf">http://dockr.ly/2xKafPf</a></li>
        </ul>
        <aside class='notes'>
            <ul>
                <li>additional resources about creating images for Linux</li>
            </ul>
        </aside>
    </section>

    <!--
    <section class="no_bg">
        <h2>Further Reading 2/2</h2>
        <h3>Windows Containers:</h3>
        <ul>
            <li>Dockerfile on Windows: <a href="http://bit.ly/2waNvsS"><span class="link">http://bit.ly/2waNvsS</span></a></li>
            <li>Optimize Windows Dockerfiles: <a href="http://bit.ly/2whpfn7"><span class="link">http://bit.ly/2whpfn7</span></a></li>
            <li>Windows Container Samples:
                <ul>
                    <li> <a href="http://bit.ly/2wCrPXy"><span class="link">http://bit.ly/2wCrPXy</span></a></li>
                    <li> <a href="http://bit.ly/2ghRr5o"><span class="link">http://bit.ly/2ghRr5o</span></a></li>
                </ul>
            </li>
            <li>Powershell Tricks: <a href="http://bit.ly/2wb7Azn"><span class="link">http://bit.ly/2wb7Azn</span></a></li>
            <li>Multi-stage builds for Windows containers: <a href="http://bit.ly/2iBRmdN"><span class="link">http://bit.ly/2iBRmdN</span></a></li>
            <li>The SHELL command: <a href="http://dockr.ly/2whvyqZ"><span class="link">http://dockr.ly/2whvyqZ</span></a></li>
        </ul>
        <aside class='notes'>
            <ul>
                <li>additional resources about creating images for Windows</li>
            </ul>
        </aside>
    </section>
    -->
</section>
<section>
    <section class="no_bg">
        <h2>Docker Volumes</h2>
    </section>

    <section class="no_bg">
        <h2>Discussion: Managing Data</h2>

        <p>If a container generates a lot of data, where should it be stored? What if you need to provision data to a container?</p>

        <aside class='notes'>
            <ul>
                <li>Guide the class to thinking about the fact that so far, containers and images don't provide a practical way to manage data that lives longer than the lifetime of a container.</li>
                <li>Hint questions if the class is stuck:</li>
                <li>Should you write a lot of data to the container layer? That's currently the only place we've learned about in this workshop where you can write data to at container run time. Why wouldn't you want to do this?</li>
                <li>Should you provision data to a container by including that data in the underlying image? Again, this is the only option we've seen so far. Why wouldn't you want to do this?</li>
            </ul>
        </aside>

    </section>

    <section class="no_bg">
        <h2>Learning Objectives</h2>

        <p>By the end of this module, learners will be able to</p>
        <ul>
            <li>Define a volume and identify its primary use cases</li>
            <li>Describe the advantages and potential security risks of mounting volumes and host directories into containers</li>
        </ul>
    </section>

    <section class="no_bg">
        <h2>Volume Usecases</h2>

        <p>Volumes provide a R/W path <span class='keyword'>separate from the layered filesystem</span>.</p>

        <ul>
            <li><span class='keyword'>Mount</span> data at container startup</li>
            <li><span class='keyword'>Persist</span> data when a container is deleted</li>
            <li><span class='keyword'>Share</span> data between containers</li>
            <li><span class='keyword'>Speed up</span> I/O by circumventing the union filesystem</li>
        </ul>

        <aside class="notes">
            <ul>
                <li>Volumes primarily provide a way to handle data that has a longer lifecycle than an individual container, by providing a writable location separate from the container's union filesystem.</li>
                <li>For example, if a container needs access to a large body of files, those files can be mounted into a running container as a volume, avoiding the need to create a (potentially huge) image with that data baked in.</li>
                <li>If a container is creating or collecting data as it runs, it should be stored in a volume, since that volume will survive the deletion of the container.</li>
                <li>Furthermore, volumes can be a more performant choice for write heavy workloads for the same reason. Rather than searching the layers of the union filesystem and performing a copy on write operation when writing a file, I/O in a volume simply reads and writes the relevant file, without the added overhead.</li>
            </ul>
        </aside>
    </section>

    <section class="no_bg">
        <h2>Basic Volumes</h2>

        <ul>
            <li><span class='keyword'>Named</span>: managed by Docker; filesystem independent; user-specified identifier</li>
            <li><span class='keyword'>Anonymous</span>: managed by Docker; filesystem independent; randomly-generated identifier</li>
            <li><span class='keyword'>Host mounted</span>: mount a specific path on the host; DIY management</li>
        </ul>

        <aside class='notes'>
            <ul>
                <li>There are two high-level methods for mounting volumes into containers: named volumes, and host mounts.</li>
                <li>In the named volume case, Docker will create and manage a directory on your host for your volume, using the 'docker volume' CLI commands.</li>
                <li>Host mounts specify a particular path on the host to mount into a container. This is useful if there's something host specific you want to provide to the container, but requires you to manage the host's filesystem directly and depends on the path in question actually being meaningful on that host, potentially affecting portability.</li>
            </ul>
        </aside>

    </section>

    <section data-background="#340B65" class="green_bg">
        <h2><img src="src/modules/Training-for-Containerization/05-volumes/images/icon_task.png" class="moby_icon" alt="icon"> Instructor Demo: Volumes</h2>

        <p>See the demo</p> 
        
        <ul>
            <li class='demo' script='basic-volume-usage-demo.md'>Basic Volume Usage</li>
        </ul>

        <p>In the Exercises book.</p>
    </section> 

    <section class="no_bg">
        <h2>Volumes in dockerfiles</h2>
        <ul>
            <li>VOLUME instruction creates a mount point</li>
            <li>Can specify arguments in a JSON array or string</li>
            <li>Cannot map volumes to host directories</li>
            <li>Volumes are initialized when the container is executed</li>
        </ul>
        <br>
        <br>
        <div class="row">
            <div class="col-6">
                <div class="pre">FROM nginx:latest
...
# string example
VOLUME /myvolume

# string example with multiple volumes
VOLUME /www/website1 /www/website2

# JSON example
VOLUME ["myvol1", "myvol2"]
...</div>
            </div>
            <!--
            <div class="col-6">
                Windows containers:<br>
                <div class="pre">FROM microsoft/iis:nanoserver
...
# string examples
VOLUME c:\\data
VOLUME c:\\data2 c:\\data3

# JSON examples
VOLUME ["c:\\\\data4", "c:\\\\data5"]
VOLUME ["c:/data6", "c:/data7"]
...</div>
            </div>
        -->
        </div>
        <aside class="notes">
            <ul>
                <li>VOLUME is another Dockerfile instruction available to designate a directory inside a container as a volume to be persisted on the host.</li>
                <li>Note that this syntax does not allow for the specification of a host path, since that would break our all-important portability; there's no guarantee that path will exist on any arbitrary machine in a meaningful way.</li>
                <li>Docker automatically creates a volume (directory) on the host for each volume that is declared in the Dockerfile. The name of such a host volume is a sha256. The data that is then stored inside those volumes inside the container is mapped/persisted to the host FS.</li>
            </ul>
        </aside>
    </section>

    <section class="no_bg">
        <h2>Volumes and Security</h2>

        <ul>
            <li>Point of ingress to the host and other containers</li>
            <li>Don't mount things unnecessarily</li>
            <li>Use the <code>:ro</code> flag</li>
            <li>Linux: in-memory <code>tmpfs</code> mounts available</li>
        </ul>

        <aside class='notes'>
            <ul>
                <li>Volumes are the first thing we've seen so far that pierce the isolation we carefully crafted between containers, the host, and other containers. A malicious actor can inject files from one container to another, and to the host, if care is not taken.</li>
                <li>Mount volumes and directories only as needed, and use the :ro flag when a container is only a passive consumer of data. Better yet, ask if there's a way to separate out reader and writer containers, to keep write access to volumes as tightly restricted as possible.</li>
                <li>Also for those running linux hosts, it's also possible to mount purely in-memory volumes to a container; anything written here will never be written to disk, and will be released when the container is deleted. This is a good option for persisting sensitive data while a container is running.</li>
            </ul>
        </aside>

    </section>


    <section data-background="#00a2a1" class="green_bg">
        <h2><img src="src/modules/Training-for-Containerization/05-volumes/images/icon_task.png" class="moby_icon" alt="icon"> Exercise: Volumes Usecase</h2>

        <p>Work through</p> 
        <ul>
            <li class='exercise' script='database-volumes.md'>Database Volumes</li>
        </ul>

        <p>In the Exercises book.</p>
        <h2 class="timer"></h2>

    </section> 

    <section class="no_bg">
        <h2>Docker Volume Takeaways</h2>
        <ul>
            <li>Volumes persist data beyond the container lifecycle</li>
            <li>Volumes bypass the copy on write system (better for write-heavy containers)</li>
        </ul>
        <aside class="notes">
            <ul>
                <li>The most important take home message for volumes, is that this is where persistent data should go - not in containers, which come and go rapidly.</li>
                <li>Also, volumes are separate from the union file system; changes to volumes do not precipitate changes to images, and vice versa.</li>
            </ul>
        </aside>
    </section>

    <section class="no_bg">
        <h2>Further Reading</h2>

        <ul>
            <li>How to use volumes: <a href="http://dockr.ly/2vRZBDG">http://dockr.ly/2vRZBDG</a></li>
            <li>Troubleshoot volume errors: <a href="http://dockr.ly/2vyjvbP">http://dockr.ly/2vyjvbP</a></li>
            <li>Docker volume reference: <a href="http://dockr.ly/2ewrlew">http://dockr.ly/2ewrlew</a></li>
        </ul>

        <aside class='notes'>
            <ul>
                <li>additional resources about volumes</li>
            </ul>
            
        </aside>
    </section>

</section>
<section>
    <section class="no_bg">
        <h2>Docker System Commands</h2>
        <aside class="notes">
            <ul>
                <li>So far, we've encountered commands to interact with individual images and containers; in practice however, a Docker workflow will generate a lot of containers and images. In this module, we'll introduce some tools for managing your entire collection of images and containers on a node.</li>
            </ul>
        </aside>
    </section>

    <section class="no_bg">
        <h2>Learning Objectives</h2>
        
        <p>By the end of this module, learners will be able to</p>
        <ul>
            <li>Execute clean-up commands</li>
            <li>Locate Docker system information</li>
        </ul>
    </section>

    <section class="no_bg">   
        <h2>Clean-up Commands</h2>
      
        <ul>
            <li>
                <code>docker system df</code><br>
                <pre>
TYPE           TOTAL    ACTIVE   SIZE        RECLAIMABLE
Images         39       2        9.01 GB     7.269 GB (80%)
Containers     2        2        69.36 MB    0 B (0%)
Local Volumes  0        0        0 B         0 B</pre>
            </li>
            <li><code>docker system prune</code></li>
        </ul>
        <div>
            <p>more limited...</p>
            <ul>
                <li><code>docker image prune [--filter "foo=bar"]</code></li>
                <li><code>docker container prune [--filter "foo=bar"]</code></li>
                <li><code>docker volume prune [--filter "foo=bar"]</code></li>
                <li><code>docker network prune [--filter "foo=bar"]</code></li>
            </ul>
        </div>
        <div class="topcorner"><img src="src/modules/Training-for-Containerization/06-system-commands/images/cleanup.png" alt="Cleanup the System"></div>

        <aside class="notes">
            <ul>
                <li>under heavy use the docker host might consume a load of resources or disk space. To find out with type of elements occupy how much space we can use the <code>docker system df</code> command. It tells us exactly how much space images, container and volumes currently occupy and how much of it is claimable.</li>
                <li>to claim back unused space from the Docker host we can use the command <code>docker system prune</code>. It will try to remove dangling images, stopped containers and unused volumes and networks in one go.</li>
                <li>we also have the more specialized <code>prune</code> commands that only remove unused items of the given type</li>
                <li>Prune commands can also be filtered by label in all cases, before a timestamp via 'until' for everything but volumes, and also by 'dangling' for images, for even more restricted pruning.</li>
            </ul>

            <p>image credit <a href="http://www.wiki.sc4devotion.com/src/modules/fundamentals/system-commands/src/modules/Training-for-Containerization/06-system-commands/images/4/41/Wiki_clean.png">http://www.wiki.sc4devotion.com/src/modules/fundamentals/system-commands/src/modules/Training-for-Containerization/06-system-commands/images/4/41/Wiki_clean.png</a></p>
        </aside>
    </section>

    <section class="no_bg">
        <h2>Inspect the System</h2>
        <div><code>docker system info</code></div>
        <pre>
Containers: 2
 Running: 2
 Paused: 0
 Stopped: 0
Images: 105
Server Version: 17.03.0-ee
Storage Driver: overlay2
 Backing Filesystem: extfs
 Supports d_type: true
 Native Overlay Diff: true
Logging Driver: json-file
Cgroup Driver: cgroupfs
Plugins:
 Volume: local
 Network: bridge host ipvlan macvlan null overlay
Swarm: active
 NodeID: ybmqksh6fm627armruq0e8id1
 Is Manager: true
 ClusterID: 2rbf1dv6t5ntro2fxbry6ikr3
 Managers: 1
 Nodes: 1
 Orchestration:
  Task History Retention Limit: 5
 Raft:
  Snapshot Interval: 10000
  Number of Old Snapshots to Retain: 0
  Heartbeat Tick: 1
  ...</pre>

        <div class="topcorner"><img src="src/modules/Training-for-Containerization/06-system-commands/images/spyglass.png" alt="Inspect System"></div>

        <aside class="notes">
            <ul>
                <li>We can use the <code>docker system info</code> command to get very detailed information about the current Docker host. This information includes but is not limited to images, containers, swarm mode, networks and volumes.</li>
                <li>
                    When looking at the output, can you identify:
                    <ul>
                        <li>how many images are on your machine?</li>
                        <li>What version of containerd are you running?</li>
                        <li>Whether Docker is running in swarm mode?</li>
                    </ul>
                </li>
                <li>Image from <a href="https://s3-us-west-2.amazonaws.com/nnsrc/modules/fundamentals/system-commands/src/modules/Training-for-Containerization/06-system-commands/images/spiglass.png">https://s3-us-west-2.amazonaws.com/nnsrc/modules/fundamentals/system-commands/src/modules/Training-for-Containerization/06-system-commands/images/spiglass.png</a></li>
            </ul>
        </aside>
    </section>

    <section class="no_bg">
        <h2>System Events</h2>

        <p>
            <span>Start observing with ...</span><br>
            <code>docker system events</code>
        </p>
        <p>Generate events with ...<br>
            <code style="font-size: 0.8em;">docker container run --rm alpine echo 'Hello World!'</code>
        </p>
        <pre>
2017-01-25T16:57:48.553596179-06:00 container create 30eb630790d44052f26c1081...
2017-01-25T16:57:48.556718161-06:00 container attach 30eb630790d44052f26c1081...
2017-01-25T16:57:48.698190608-06:00 network connect de1b2b40f522e69318847ada3...
2017-01-25T16:57:49.062631155-06:00 container start 30eb630790d44052f26c1081d...
2017-01-25T16:57:49.164526268-06:00 container die 30eb630790d44052f26c1081dbf...
2017-01-25T16:57:49.613422740-06:00 network disconnect de1b2b40f522e69318847a...
2017-01-25T16:57:49.815845051-06:00 container destroy 30eb630790d44052f26c108...</pre>

        <div class="topcorner" style="max-width: 35%"><img src="src/modules/Training-for-Containerization/06-system-commands/images/monitoring.png" alt="System Events"></div>
        <aside class="notes">
            <ul>
                <li>`docker system info` gave us some mostly-static metadata about our docker platform; if we want to watch things live, we can take advantage of Docker's event reporting via <code>docker system events</code>.</li>
                <li>please note that the output generated by the <code>docker system events</code> command can also be filtered and custom formatted by using according command arguments <code>--filter</code> and <code>format</code></li>
                <li>Image from <a href="http://www.bing.com/src/modules/Training-for-Containerization/06-system-commands/images/search?view=detailV2&ccid=xHDtNSak&id=F6B851CB5DF9FCB90FD4C4FBF86073617E979328&q=monitoring&simid=608004200364248188&selectedIndex=0&qft=+filterui%3alicense-L1+filterui%3aimagesize-medium&ajaxhist=0">http://www.bing.com/src/modules/fundamentals/system-commands/src/modules/Training-for-Containerization/06-system-commands/images/search?view=detailV2&ccid=xHDtNSak&id=F6B851CB5DF9FCB90FD4C4FBF86073617E979328&q=monitoring&simid=608004200364248188&selectedIndex=0&qft=+filterui%3alicense-L1+filterui%3aimagesize-medium&ajaxhist=0</a></li>
            </ul>            
        </aside>
    </section>

    <section  data-background="#00a2a1" class="green_bg">
        <h2><img src="src/modules/Training-for-Containerization/06-system-commands/images/icon_task.png" class="moby_icon" alt="icon"> Exercise: System Commands</h2>
        <p>Work through</p>
        
        <ul>
            <li class='exercise' script='cleaning-up-docker-resources.md'>Cleaning up Docker Resources</li>
            <li class='exercise' script='inspection-commands.md'>Inspection Commands</li>
        </ul>

        <p>in the Exercises book.<p>
        <h2 class="timer"></h2>

    </section> 

    <section class="no_bg">
        <h2>Discussion</h2>

        <ul>
            <li>What is the origin of dangling image layers?</li>
            <li>What are some potential pitfalls to automating system cleanup with prune commands, and how to avoid them?</li>
            <li>Questions?</li>
        </ul>

        <aside class='notes'>
            <ul>
                <li>When a layer is no longer part of any tagged image, typically happens when a name and tag is reused after changing a Dockerfile.</li>
                <li>Deleting containers without grabbing their logs first, deleting volumes that have valuable info just because they weren't currently attached to anything. Avoid by using label-based filters when pruning.</li>
            </ul>
        </aside>
    </section>

    <section class="no_bg">
        <h2>Further Reading</h2>

        <ul>
            <li>System commands reference: <a href="http://dockr.ly/2eMR53i">http://dockr.ly/2eMR53i</a></li>
        </ul>

        <aside class='notes'>
            <ul>
                <li>additional resources about system commands</li>
            </ul>
            
        </aside>
    </section>
</section>
<section class="no_bg">
    <h2>Containerization Fundamentals Conclusion: Any App, Anywhere.</h2>

    <ul>
        <li>Containers are isolated processes</li>
        <li>Images provide filesystem for containers</li>
        <li>Volumes persist data</li>
    </ul>

    <aside class='notes'>
        <ul>
            <li>the key takehome from basic containerization is an understanding of how it lets docker deliver on its promise to enable you to run any app, anywhere.</li>
            <li>the layered filesystems defined by Dockerfiles which in turn define images contain all the execution context a process needs to run; features of the linux kernel like kernel namespacing and control groups allow that environment to be created as a container on any host linux system, securely and without regard to whatever else is running on that machine. These tools provide the standardization and encapsulation we predicted would be of benefit in the introduction at the start of the day.</li>
            <li>One slightly subtler point is that in none of this did we ever impose any restrictions on how many containers could be running on a given machine, or any necessary connections between the images that underlie them. This ability to mix and match frees us from correlations between processes; run your postgres database from ubuntu, your node.js web app from debian and your FORTRAN data wrangling from centos if you want - all on the same machine. Containerization's implicit win is the ability to always use the right tool for the job.</li>
        </ul>
    </aside>

</section>

<section>
    <section class="no_bg">
        <h2>Wrap Up - Spring Boot</h2>
    </section>
    <section data-background="#00a2a1" class="green_bg">
        <h2>Exercise Instructions</h2>
        <ul>
            <li>Goal: Build a docker image that runs a Java application</li>
            <li>Clone the git repository at <a href="https://github.com/ckaserer/java-helloworld">https://github.com/ckaserer/java-helloworld</a></li>
            <li>Build the application using <code>gradle bootJar</code> and find the resulting fat-jar file at <code>build/libs</code> (already done for you).</li>
            <li>You can run the java application locally with <code>java –jar rest-service-0.0.1-SNAPSHOT.jar</code>, this starts an application server on port 8080. Check it in your browser via <a href="http://127.0.0.1:8080/greeting">http://127.0.0.1:8080/greeting</a> (respectively with your AWS instance public dns).</li>
        </ul>
        <br>
        <br>
        <h2 class="timer"></h2>
    </section>
    
    <section data-background="#00a2a1" class="green_bg">
        <h2>Solution Hints</h2>
        <ul>
            <li>Build the application outside of the container (already done and checked into git).</li>
            <li>Find a suitable docker base image, that has a JRE installation.</li>
            <li>Use both the <code>CMD</code> and the <code>ENTRYPOINT</code> instructions.</li>
            <li>Run two instances of the container – what do you have to take care of? Make sure both instances are reachable via browser.</li>
        </ul>
        <br>
        <br>
        <h2 class="timer"></h2>
    </section>
    
    <section class="no_bg">
        <h2>Solution</h2>
        <p>Sample Dockerfile</p>
        <div class='pre'>FROM anapsix/alpine-java
LABEL MAINTAINER=clemens.kaserer@gepardec.com
WORKDIR /data
EXPOSE 8080
COPY build/libs/rest-service-0.0.1-SNAPSHOT.jar \
     rest-service-0.0.1-SNAPSHOT.jar
CMD ["-jar", "rest-service-0.0.1-SNAPSHOT.jar"]
ENTRYPOINT ["java"]</div>            
        <p>Solution Commands</p>
            <ul>
                <li>docker build -t spring_boot_example .</li>
                <li>docker run –d –p 80:8080 spring_boot_example</li>
            </ul>

    </section>
</section><section class="no_bg">
    <h2>Containerization Training</h2>
    <p>Please take our feedback survey</p>
    <p>Get in touch: office@gepardec.com</p>
    <p><a href='https://www.gepardec.com/trainings'>https://www.gepardec.com/trainings</a></p>
</section>



<script>
    // Set the date we're counting down to
    
    function addMinutes(date, minutes) {
      return new Date(date.getTime() + minutes*60000);
    }

    var countDownDate = addMinutes( new Date(), 22);
    
    // Update the count down every 1 second
    var x = setInterval(function() {
    
      // Get today's date and time
      var now = new Date().getTime();
        
      // Find the distance between now and the count down date
      var distance = countDownDate - now;
        
      // Time calculations for days, hours, minutes and seconds
      var days = Math.floor(distance / (1000 * 60 * 60 * 24));
      var hours = Math.floor((distance % (1000 * 60 * 60 * 24)) / (1000 * 60 * 60));
      var minutes = Math.floor((distance % (1000 * 60 * 60)) / (1000 * 60));
      var seconds = Math.floor((distance % (1000 * 60)) / 1000);
    

      var el = document.getElementsByClassName("timer");
      for (var i = 0, ilen = el.length; i < ilen; i++) {
        el[i].innerHTML = minutes + "m " + seconds + "s ";
        // If the count down is over, write some text 
        if (distance < 0) {
          clearInterval(x);
          el[i].innerHTML = "Time is up!";
        }
      }
      
    }, 1000);
</script><section>
    <section class="no_bg">
        <h2>Docker Networking Basics</h2>

        <aside class="notes">
            <ul>
                <li>[Instructor aside: this module is intended to bridge between one day of introduction to containerization, and one day of introduction to orchestration. As such it can equally well go at the end of the first or the beginning of the second, or be omitted entirely if the workshop isn't introducing orchestration].</li>
                <li>To begin our exploration of orchestration, we first need to examine the basics of how two containers can be networked together on a single host.</li>
            </ul>
        </aside>
    </section>

    <section class="no_bg">
        <h2>Discussion: Portable Networks</h2>

        <p>Network traffic must by definition traverse a network outside its originating container. How can we make inter-container communication as portable and secure as containers themselves?</p>

        <aside class='notes'>
            <ul>
                <li>Lead the class to think about the need for networking abstractions, both software defined networks and DNS-resolvable container names.</li>
                <li>Hint questions if the class is stuck:</li>
                <li>Can we rely on a container having the same IP or mac address every time it is created, no matter what host it is created on? (obviously not, a given private IP could already be taken on a destination host, and any global public identifier like a public IP or mac address couldn't be practically registered at container run time). Therefore, we must need some sort of networking layer that abstracts away the host network. That also might give us the opportunity to impose some security on our networks, since we control this extra networking layer.</li>
                <li>How will service discovery work in our containerized application logic? We'd like to avoid having a lot of boilerplate that exposes the networking underlay to our application logic; Docker should provide some portable addressing mechanism. (leads to thinking about DNS).</li>
            </ul>
        </aside>

    </section>

    <section class="no_bg">
        <h2>Learning Objectives</h2>

        <p>By the end of this module, learners will be able to</p>
        <ul>
            <li>Describe Docker's container network model and its security implications</li>
            <li>Describe the basic technologies that underwrite single host networks</li>
            <li>Understand how Docker manipulates a host's firewall rules to control container traffic</li>
        </ul>
    </section>

    <section class="no_bg">
        <h2>The Container Network Model</h2>

        <img src='src/modules/Training-for-Containerization/10-networking-basics/images/cnm-simplified.png'></img>

        <aside class='notes'>
            <ul>
                <li>At high level, docker thinks about networking with an abstraction called the Container Network Model (CNM) that consists of 3 parts:</li>
                <li>The container (network) sandbox, which firewalls containers by default.</li>
                <li>The network endpoint, which serves as a controlled port in and out of the container sandbox</li>
                <li>The network itself, which is any device that facilitates inter-container communication.</li>
                <li>If you think about it for a moment, the CNM is in some sense very vague; anything that satisfies these requirements is a valid implementation option. Just like we saw with containerization itself, Docker leverages battle-tested kernel features and linux tools to realize the CNM in practice.</li>
            </ul>
        </aside>

    </section>

    <section class="no_bg">
        <h2>Linux: Default Single-Host Network</h2>

        <img src='src/modules/Training-for-Containerization/10-networking-basics/images/default-network-1.png'></img>

        <aside class='notes'>
            <ul>
                <li>When Docker is started on the host, a linux bridge is created by default, and assigned an unused private subnet from 172.[17-31].0.0/16 or 192.168.[0-240].0/20. A linux bridge is an in-software switch, that routes packets by MAC address.</li>
            </ul>
        </aside>

    </section>

    <section class="no_bg">
        <h2>Linux: Default Container Networking</h2>

        <img src='src/modules/Training-for-Containerization/10-networking-basics/images/default-network-2.png'></img>
        <p><span class='keyword'>Quiz:</span> identify the sandbox, endpoint and network corresponding to the container networking model objects in this diagram.</p>

        <aside class='notes'>
            <ul>
                <li>When a container is created, the container networking model must be satisfied. Containers run in their own network namespace, satisfying the sandboxing requirement; inside the namespace, processes won't be able to access host networking devices unless explicitly connected to them.</li>
                <li>In order to connect a container to the rest of the system, a virtual ethernet (veth) pair is created, with one endpoint connected to the default docker bridge, and the other presented as an ethX port inside the container with a private IP taken from the bridge's subnet. Veth connections operate as a pipe, forwarding all traffic in one end to the other, even across network namespaces.</li>
            </ul>
        </aside>

    </section>

    <section class="no_bg">
        <h2>Linux: User-Defined Bridges &amp; Firewalls</h2>

        <img src='src/modules/Training-for-Containerization/10-networking-basics/images/user-bridge.png'></img>

        <aside class='notes'>
            <ul>
                <li>Users can optionally create their own linux bridges, and plug containers into them for communicating on the same host.</li>
                <li>Containers on user-created bridge networks can resolve each other by container name; containers names are registered in the Docker daemon's DNS table for resolution.</li>
                <li>You'll build something like this in the next exercise.</li>
            </ul>
        </aside>
    </section>
<!--
    <section class="no_bg">
        <h2>Windows: The nat Network</h2>

        <img src='src/modules/Training-for-Containerization/10-networking-basics/images/win-nat.png'></img>

        <aside class='notes'>
            <ul>
                <li>While the windows networking stack is completely different, many of its components follow a close analogy to the linux stack; both use network namespaces, for example, to satisfy the sandboxing requirement of the docker network model.</li>
                <li>Windows hosts use a Hyper-V virtual switch combined with the WinNAT network address translation service to provide the same layer-2 routing that a linux bridge does; this serves the role of the docker software-defined network in the network model.</li>
                <li>Endpoints in windows are provided by ports in the virtual switch connected to virtual NICs in windows server containers, or VM NICs inside hyper-v containers.</li>
                <li>Windows Firewall rules provide similar constraints on container communication as in linux.</li>
                <li>Note that the default windows NAT network is deleted automatically on reboot in Windows Server 2019; Docker will recreate a NAT network for itself, but any containers plugged into a previous NAT will get new IPs after a reboot.</li>
            </ul>
        </aside>
    </section>
-->
    <section class="no_bg">
        <h2>Exposing Container Ports</h2>

        <ul>
            <li>Containers have no public IP address by default.</li>
            <li>Can forward host port -> container port</li>
            <li>Mapping created manually or automatically.</li>
            <li>Port mappings visible via <br><code>docker container ls</code> or <br><code>docker container port</code></li>
        </ul>

        <aside class='notes'>
            <ul>
                <li>An axiom of Docker security can be thought of as 'isolated by default'</li>
                <li>In terms of networking, this implies that containers are not reachable from the outside world by default.</li>
                <li>Port mappings have to be set up if a containerized process is to be reachable directly.</li>
            </ul>
        </aside>

    </section>

    <section data-background="#340B65" class="green_bg">
        <h2><img src="src/modules/Training-for-Containerization/10-networking-basics/images/icon_task.png" class="moby_icon" alt="icon"> Instructor Demo: Single Host Networks</h2>

        <p>See the demo</p> 
        
        <ul>
            <li class='demo' script='single-host-network-demo.md'>Single Host Networks</li>
        </ul>

        <p>In the Exercises book.</p>
    </section> 

    <section data-background="#00a2a1" class="green_bg">
        <h2><img src="src/modules/Training-for-Containerization/10-networking-basics/images/icon_task.png" class="moby_icon" alt="icon"> Exercise: Single Host Networks</h2>
        <p>Work through</p>

        <ul>
            <li class='exercise' script='introduction-to-container-networking.md'>Introduction to Container Networking</li>
            <li class='exercise' script='container-port-mapping.md'>Container Port Mapping</li>
        </ul>

        <p>in the Exercises book.</p>
        <h2 class="timer"></h2>
    </section> 

    <section class="no_bg">
        <h2>Docker Networking Takeaways</h2>

        <ul>
            <li>
                Single host networks follow the container networking model:
                <ul>
                    <li>Sandbox: Network namespaces</li>
                    <li>Endpoint: veth (linux)</li>
                    <li>Network: bridge (linux)</li>
                </ul>
            </li>
            <li>Containers resolve each other by DNS lookup when explicitly named and attached to custom networks</li>
            <li>Docker software defined networks are firewalled from each other by default</li>
        </ul>

        <aside class='notes'>
            <ul>
                <li>The key takeaway for Docker container networking is the paradigm of isolation by default. Containers must be explicitly connected to the same network to talk to each other; leverage this to easily improve the security of your applications.</li>
                <li>Similarly, containers are not exposed on the external network by default; they must explicitly have ports mapped to the host if they are to be reachable by the outside world. Do not expose or map ports unnecessarily, as this leads to port conflicts and security risks!</li>
                <li>For much more detail, see the corresponding reference architecture linked below.</li>
            </ul>
        </aside>

    </section>

    <section class="no_bg">
        <h2>Further Reading</h2>
        <ul>
            <li>Docker Reference Architecture: Designing Scalable, Portable Docker Container Networks: <a href="https://dockr.ly/2q3O8jq">https://dockr.ly/2q3O8jq</a></li>
            <li>Network containers: <a href="http://dockr.ly/2x1BYgW">http://dockr.ly/2x1BYgW</a></li>
            <li>Docker container networking: <a href="http://dockr.ly/1QnT6y8">http://dockr.ly/1QnT6y8</a></li>
            <li>Understand container communication: <a href="http://dockr.ly/2iSrHO0">http://dockr.ly/2iSrHO0</a></li>
        </ul>

        <aside class='notes'>
            <ul>
                <li>additional resources about networking basics</li>
            </ul>
            
        </aside>
    </section>
</section>
<section>
    <section class="no_bg">
        <h2>Introduction to Docker Compose</h2>
        <aside class="notes">
            <ul>
                <li>Docker provides a number of fundamental tools for approaching the problem of orchestration natively from Docker, and the first of these tools is Compose.</li>
            </ul>
        </aside>
    </section>

    <section class="no_bg">
        <h2>Discussion: Processes vs. Applications</h2>

        <p>Containers and images describe individual processes. What will we need to describe entire applications?</p>

        <aside class='notes'>
            <ul>
                <li>Lead class to the simple answer (some sort of manifest file that describes the application, in the spirit of deploy scripts or infrastructure-as-code), as well as the more substantial answer of some way to manage a living application, where 'manage' in this context means scale, route traffic, deploy and upgrade.</li>
                <li>Hint questions if the class is stuck:</li>
                <li>It's not enough just to 'describe' an application - we need to make the deployment of those applications reproducible and portable. How? (someone should think of some sort of script).</li>
                <li>Are applications static after they're launched? What if load changes? (leads to thinking about scaling).</li>
                <li>After scaling an application, how do we make sure traffic gets to the new instances of our app? Re-do service discovery? Reconfigure load balancers? We'd rather have something a little more transparent.</li>
            </ul>
        </aside>

    </section>

    <section class="no_bg">
        <h2>Learning Objectives</h2>

        <p>By the end of this module, learners will be able to</p>
        <ul>
            <li>Design scalable Docker services</li>
            <li>Leverage Docker's built in service discovery mechanism</li> 
            <li>Write a compose file describing an application</li>
        </ul>

    </section>

    <section class="no_bg">
        <h2>Distributed Application Architecture</h2>

        <ul>
            <li>Applications consisting of one or more containers across one or more nodes</li>
            <li>Docker Compose facilitates multi-container design <span class='keyword'>on a single node</span></li>
        </ul>

        <aside class="notes">
            <ul>
                <li>At this point, we've seen that Docker can provide adequately portable and isolated containers, and we've seen some basic nuts and bolts regarding how those containers can be networked together; we're now ready to start exploring our first orchestration tool for making a true distributed application.</li>
                <li>Ultimately, we'll want to be able to completely decentralize our application, be networking many containers together across many hosts; for now, we'll just solve half the problem, by making an application out of many containers, still all on the same host. We'll relax the single-host constraint in the next chapter.</li>
            </ul>
        </aside>
    </section>

    <section class="no_bg">
        <h2>Docker Services</h2>

            <ul>
                <li>Goal: declare and (re)configure many similar containers all at once</li>
                <li>Goal: scale apps by adding containers seamlessly</li>
                <li>A <span class='keyword'>service</span> defines the <span class='keyword'>desired state</span> of a group of identically configured containers</li>
                <li>Docker provides <span class='keyword'>transparent service discovery</span> for Services</li>
            </ul>

            <aside class='notes'>
                <ul>
                    <li>So far, we've declared containers one at a time with `docker container run...`, and we've seen how to network individual containers together. This all works, but doesn't scale conveniently.</li>
                    <li>Since we're going to start designing apps to consist of potentially many containers, we'd like to be able to create and reconfigure containers en masse.</li>
                    <li>Furthermore, we need to put some thought into how discovery will work in this paradigm; if we scale up an app by declaring more containers, how will they all find out about each other and network themselves together</li>
                    <li>To address this problem, Docker orchestration introduces the idea of services. A service defines the desired state of a collection of identically configured containers, allowing us to declare a batch of containers all at once, and reconfigure them later by updating the service definition.</li>
                    <li>Furthermore, Docker provides out-of-the-box service discovery for services, automatically providing and configuring the networking necessary for these groups of containers to interact.</li>
                </ul>
            </aside>
    </section>  

    <section class="no_bg">
        <h2>Service Discovery</h2>

        <img src='src/modules/Training-for-Kubernetes/03-compose/images/service-not-process.png' style='width:90%'></img>
        <p>Services are assigned a <span class='keyword'>Virtual IP</span> which spreads traffic out across the underlying containers automatically.</p>

        <aside class='notes'>
            <ul>
                <li>Formerly, we may have had individual processes or containers communicating directly; this isn't practical for a service we want to scale into many processes on demand.</li>
                <li>To address this, Docker assigns a virtual IP to every service, and maintains a DNS lookup table on the host, so that at the application logic level, traffic can be directed to a service as a whole; load balancing to the underlying containers is handled by Docker's onboard VIP server.</li>
                <li>In this way, we can change the number of containers provisioned by a service without needing to do any explicit service discovery in our applications; the application logic sends traffic to the service regardless of how many containers it has provisioned, and Docker does the rest.</li>
            </ul>
        </aside>
    </section>    

    <section class="no_bg">
        <h2>Our Application: Dockercoins</h2>

        <div class='col-6'>
            <img style="background-color:rgba(0,0,0,0); max-width:80%;" src="src/modules/Training-for-Kubernetes/03-compose/images/dockercoins.png" alt="DockerCoins logo" />
            <p style='font-size: medium !important'>
                (DockerCoins 2016 logo courtesy of <a href="https://twitter.com/xtlcnslt">@XtlCnslt</a> and <a href="https://twitter.com/ndeloof">@ndeloof</a>. Thanks!)
            </p>
        </div>

        <div class='col-6'>
            <ul>
                <li>
                    It is a DockerCoin miner! 💰🐳📦🚢
                </li>
                <li>
                    Dockercoins consists of 5 services working together:
                </li>
            </ul>
            <img src='src/modules/Training-for-Kubernetes/03-compose/images/dockercoins-flow.png' style='width:70%'></img>
        </div>
    </section>

    <section data-background="#340B65" class="green_bg">
        <h2><img src="src/modules/Training-for-Kubernetes/03-compose/images/icon_task.png" class="moby_icon" alt="icon"> Instructor Demo: Docker Compose</h2>

        <p>See the demo</p> 
        
        <ul>
            <li class='demo' script='docker-compose-demo.md'>Docker Compose</li>
        </ul>

        <p>In the Exercises book.</p>
    </section>

    <section data-background="#00a2a1" class="green_bg">
        <h2><img src="src/modules/Training-for-Kubernetes/03-compose/images/icon_task.png" class="moby_icon" alt="icon"> Exercise: Compose Apps</h2>
        <p>Work through</p>
        <ul>
            <li class='exercise' script='starting-a-compose-app.md'>Starting a Compose App</li>
            <li class='exercise' script='scaling-a-compose-app.md'>Scaling a Compose App</li>
        </ul>

        <p>in the Exercises book.</p>
        <h2 class="timer"></h2>
    </section> 

    <section class="no_bg">
        <h2>Docker Compose Takeaways</h2>

        <ul>
            <li>Docker Compose makes single node orchestration easy</li>
            <li>Compose services makes scaling applications easy</li>
            <li>Bottleneck identification important</li>
            <li>Syntactically: <code>docker-compose.yml</code> + API</li>
        </ul>

        <aside class='notes'>
            <ul>
                <li>Compose should be your go-to native Docker solution for orchestrating services and containers on a single node.</li>
                <li>In the next section, we'll learn how to do the same across many nodes.</li>
            </ul>
        </aside>

    </section>

    <section class="no_bg">
        <h2>Further Reading</h2>

        <ul>
            <li>Docker compose examples: <a href="http://dockr.ly/1FL2VQ6">http://dockr.ly/1FL2VQ6</a></li>
            <li>Overview of docker-compose CLI: <a href="http://dockr.ly/2wtQlZT">http://dockr.ly/2wtQlZT</a></li>
            <li><code>docker-compose.yaml</code> reference: <a href="http://dockr.ly/2iHUpeX">http://dockr.ly/2iHUpeX</a></li>
            <li>Docker Compose and Windows: <a href="http://bit.ly/2watrqk">http://bit.ly/2watrqk</a></li>
        </ul>

        <aside class='notes'>
            <ul>
                <li>additional resources about Docker compose</li>
            </ul>
            
        </aside>
    </section>
</section><section>
    <section class="no_bg">
        <h2>Wrap Up Docker-Compose - Sonarqube</h2>
    </section>
    <section class="no_bg">
        <h2>Sonarqube</h2>
        <img src='src/modules/Training-for-Kubernetes/04-wrapup-compose/images/sonarqube.png' style='height: 400px' class="transparent"></img>
    </section>
    <section data-background="#00a2a1" class="green_bg">
        <h2>Exercise Instructions</h2>
        <ul>
            <li>Setup a Sonarqube server that listens on port 9000</li>
            <li>Connect it to a persistent database<br>i.e. if you ‚docker rm –f‘ your Sonarqube container and run a new one, no data is lost</li>
            <li>Use postgresql and persist it‘s data on the host filesystem using volumes</li>
            <li>Verify e.g. by creating a user via Sonarqube UI, remove the container and run a new one – is the user still present?</li>
            <li>Check that Sonarqube is really using your postgresql database
            </li>
            <li><b>Hint</b>: use docker-compose</li>
        </ul>
        <br>
        <br>
        <h2 class="timer"></h2>
        <aside class="notes">
            Check that Sonarqube is really using your postgresql database: 
                <ul>
                    <li>login to postgres container</li>
                    <li>psql sonar sonar</li>
                    <li>\l - list all existing databases</li>
                    <li>\dt - list all tables of databases</li>
                    <li><b>Hint</b>: Postgresql tips at http://www.unixwitch.de/de/sysadmin/tools/postgres</li>
                </ul>
        </aside>
    </section>
    
    <section class="no_bg">
        <h2>Solution</h2>
        <div class='pre' style="font-size:small;">version: "2"
services:
  sonarqube:
    image: sonarqube
    ports:
      - "9000:9000"
    networks:
      - sonarnet
    environment:
      - SONARQUBE_JDBC_URL=jdbc:postgresql://db:5432/sonar
    volumes:
      - sonarqube_conf:/opt/sonarqube/conf
      - sonarqube_data:/opt/sonarqube/data
      - sonarqube_extensions:/opt/sonarqube/extensions
      - sonarqube_bundled-plugins:/opt/sonarqube/lib/bundled-plugins

  db:
    image: postgres
    networks:
      - sonarnet
    environment:
      - POSTGRES_USER=sonar
      - POSTGRES_PASSWORD=sonar
    volumes:
      - postgresql:/var/lib/postgresql
      - postgresql_data:/var/lib/postgresql/datanetworks:
    sonarnet:
      driver: bridge

volumes:
  sonarqube_conf:
  sonarqube_data:
  sonarqube_extensions:
  sonarqube_bundled-plugins:
  postgresql:
  postgresql_data:</div>

    </section>
</section>
            </div>
        </div>

        <script>

            // Full list of configuration options available at:
            // https://github.com/hakimel/reveal.js#configuration
            Reveal.initialize({
                
				hash: true,

				// Learn about plugins: https://revealjs.com/plugins/
				plugins: [ RevealMarkdown, RevealHighlight, RevealNotes ]

            });

        </script>

    </body>
</html>